
@article{grossVariabilitySignificanceClass2013,
	title = {The variability and significance of class characteristics in footwear impressions},
	volume = {63},
	number = {3},
	journal = {Journal of Forensic Identification},
	author = {Gross, Susan and Jeppesen, Dane and Neumann, Cedric},
	year = {2013},
	pages = {332},
	file = {Gross et al_2013_The variability and significance of class characteristics in footwear.pdf:/home/srvander/Zotero/storage/EC9IV5R5/Gross et al_2013_The variability and significance of class characteristics in footwear.pdf:application/pdf}
}

@book{bodziakfootwear2000,
	address = {Boca Raton, Florida},
	title = {Footwear {Impression} {Evidence}: {Detection}, {Recovery}, and {Examination}},
	isbn = {0-8493-1045-8},
	publisher = {CRC Press},
	author = {Bodziak, William J.},
	year = {2000}
}

@inproceedings{alexanderAutomaticClassificationRecognition1999,
	title = {Automatic classification and recognition of shoeprints},
	volume = {2},
	doi = {10.1049/cp:19990401},
	abstract = {The most common clues left at a crime scene when a crime is committed are shoeprint impressions. These impressions are useful in the detection of criminals and the linking of crime scenes. We are currently working on a fully automatic system utilising novel fractal pattern matching techniques which allow the investigating officer(s) to match collected impressions against a database of known shoeprint patterns. There is currently no other shoeprint impression database available that offers this facility.},
	booktitle = {Image {Processing} {And} {Its} {Applications}, 1999. {Seventh} {International} {Conference} on ({Conf}. {Publ}. {No}. 465)},
	author = {Alexander, A and Bouridane, A and Crookes, D},
	month = jul,
	year = {1999},
	keywords = {image matching, recognition, automatic classification, collected impressions, crime scene, criminals, fractal pattern matching techniques, fully automatic system, shoeprints},
	pages = {638--641 vol.2}
}


@inproceedings{pavlouAutomaticExtractionClassification2006,
	title = {Automatic extraction and classification of footwear patterns},
	booktitle = {International {Conference} on {Intelligent} {Data} {Engineering} and {Automated} {Learning}},
	publisher = {Springer},
	author = {Pavlou, Maria and Allinson, Nigel M},
	year = {2006},
	pages = {721--728}
}

@article{rathinavelFullShoePrint2011,
	title = {Full {Shoe} {Print} {Recognition} based on {Pass} {Band} {DCT} and {Partial} {Shoe} {Print} {Identification} using {Overlapped} {Block} {Method} for {Degraded} {Images}},
	volume = {26},
	issn = {09758887},
	url = {http://www.ijcaonline.org/volume26/number8/pxc3874301.pdf},
	doi = {10.5120/3126-4301},
	abstract = {In this paper, a novel approach is made to discard the degradations in full shoe prints and partial shoe prints, in processing those images for recognition. A pass band DCT coefficient has been used to extract feature vectors. A more robust approach has been dealt with to find the matching between the partial shoe prints and the images in the data base. This method makes the shoe print recognition process more robust against degradations like noises, orientations and blurred images which are common in shoe print images and also helps in saving the processing time and memory consumption.},
	language = {en},
	number = {8},
	urldate = {2019-03-05},
	journal = {International Journal of Computer Applications},
	author = {Rathinavel, S. and Arumugam, S.},
	month = jul,
	year = {2011},
	pages = {16--21}
}

@inproceedings{cervelliTranslationalRotationalInvariant,
	address = {Aalborg, Denmark},
	title = {A {Translational} {And} {Rotational} {Invariant} {Descriptor} {For} {Automatic} {Footwear} {Retrieval} {Of} {Real} {Cases} {Shoe} {Marks}},
	abstract = {Shoe marks found on the crime scene are invaluable for the identiﬁcation of the culprit when no other piece of evidence is available. Thus semi-automatic and automatic systems have been recently proposed to ﬁnd the make and model of the footwear that left the shoe marks. The systems proposed up to now have two main drawbacks, as they (i) are generally not based on rotation and translation invariant descriptions, and (ii) are tested on synthetic shoe marks, i.e. on shoeprints with added synthetic noise. Here we show the results of a translation and rotation invariant description based on the Fourier transform properties: the test is made on both synthetic and real shoe marks and a comparison with algorithms proposed by others is presented.},
	language = {en},
	publisher = {EURASIP},
	author = {Cervelli, Federico and Dardi, Francesca and Carrato, Sergio},
	pages = {1665--1669}
}

@inproceedings{guehamAutomaticRecognitionShoeprints2008,
	title = {Automatic {Recognition} of {Shoeprints} using {Fourier}-{Mellin} {Transform}},
	doi = {10.1109/AHS.2008.48},
	abstract = {This paper proposes a technique for automatically recognising shoeprint images for use in forensic science. The method uses the Fourier-Mellin transform to produce translation, rotation and scale invariant features. A two dimensional correlation is employed as the similarity metric for the classification process. Experiments were conducted on a database of 500 different shoeprint images representing a part of available shoes on the market. In order to test the robustness of the method, test images including different perturbations such as noise addition and cropping (partial shoeprints) were generated. Experimental results show that the proposed method is very practical providing attractive performance when processing distorted shoeprint images.},
	booktitle = {2008 {NASA}/{ESA} {Conference} on {Adaptive} {Hardware} and {Systems}},
	author = {Gueham, M. and Bouridane, A. and Crookes, D. and Nibouche, O.},
	month = jun,
	year = {2008},
	pages = {487--491}
}


@incollection{krizhevskyImageNetClassificationDeep2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	urldate = {2019-03-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105}
}

@inproceedings{zhangAdaptingConvolutionalNeural2017,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Adapting {Convolutional} {Neural} {Networks} on the {Shoeprint} {Retrieval} for {Forensic} {Use}},
	isbn = {978-3-319-69923-3},
	abstract = {Shoeprint is an important evidence for crime investigation. Many automatic shoeprint retrieval methods have been proposed in order to efficiently provide useful information for the identification of the criminals. In the mean time, the convolutional neural network shows great capacity in image classification problem but its application in shoeprint retrieval is not yet investigated. This paper presents an application of VGG16 network as feature extractor in shoeprint retrieval and a data augmentation method to fine-tune the neural network with a very small database. Our method shows a much better performance compared with state-of-the-art methods on a same database with crime-scene-like shoeprints.},
	language = {en},
	booktitle = {Biometric {Recognition}},
	publisher = {Springer International Publishing},
	author = {Zhang, Yang and Fu, Huanzhang and Dellandréa, Emmanuel and Chen, Liming},
	editor = {Zhou, Jie and Wang, Yunhong and Sun, Zhenan and Xu, Yong and Shen, Linlin and Feng, Jianjiang and Shan, Shiguang and Qiao, Yu and Guo, Zhenhua and Yu, Shiqi},
	year = {2017},
	keywords = {Feature extraction, Convolutional neural network, Deep learning, Fine-tuning, Shoeprint retrieval},
	pages = {520--527}
}

@article{kongCrossDomainImageMatching2019a,
	title = {Cross-{Domain} {Image} {Matching} with {Deep} {Feature} {Maps}},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1804.02367},
	doi = {10.1007/s11263-018-01143-3},
	abstract = {We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene. This recognition problem is made difficult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns. We find that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for this specialized domains. However, the choice of similarity measure for matching exemplars to a query image is essential to good performance. For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness. Our proposed metric significantly improves performance in matching crime scene shoeprints to laboratory test impressions. We also show its effectiveness in other cross-domain image retrieval problems: matching facade images to segmentation labels and aerial photos to map images. Finally, we introduce a discriminatively trained variant and fine-tune our system through our proposed metric, obtaining state-of-the-art performance.},
	urldate = {2019-03-14},
	journal = {International Journal of Computer Vision},
	author = {Kong, Bailey and Supancic, James and Ramanan, Deva and Fowlkes, Charless C.},
	month = jan,
	year = {2019}
}

@article{kongCrossDomainForensicShoeprint2017,
	title = {Cross-{Domain} {Forensic} {Shoeprint} {Matching}},
	abstract = {We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene. This recognition problem is made difﬁcult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns. We ﬁnd that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for these specialized domains. However, the choice of similarity measure for matching exemplars to a query image is essential to good performance. For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness. Finally, we introduce a discriminatively trained variant and ﬁne-tune our system end-to-end, obtaining state-of-the-art performance.},
	language = {en},
	journal = {British Machine Vision Conference},
	author = {Kong, Bailey and Supancic, James and Ramanan, Deva and Fowlkes, Charless},
	year = {2017},
	pages = {17},
}

@article{chazalAutomatedProcessingShoeprint2005,
	title = {Automated processing of shoeprint images based on the {Fourier} transform for use in forensic science},
	volume = {27},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2005.48},
	abstract = {The development of a system for automatically sorting a database of shoeprint images based on the outsole pattern in response to a reference shoeprint image is presented. The database images are sorted so that those from the same pattern group as the reference shoeprint are likely to be at the start of the list. A database of 476 complete shoeprint images belonging to 140 pattern groups was established with each group containing two or more examples. A panel of human observers performed the grouping of the images into pattern categories. Tests of the system using the database showed that the first-ranked database image belongs to the same pattern category as the reference image 65 percent of the time and that a correct match appears within the first 5 percent of the sorted images 87 percent of the time. The system has translational and rotational invariance so that the spatial positioning of the reference shoeprint images does not have to correspond with the spatial positioning of the shoeprint images of the database. The performance of the system for matching partial-prints was also determined.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chazal, P. de and Flynn, J. and Reilly, R. B.},
	month = mar,
	year = {2005},
	pages = {341--350}
}


@article{luostarinenMeasuringAccuracyAutomatic2014a,
	title = {Measuring the {Accuracy} of {Automatic} {Shoeprint} {Recognition} {Methods}},
	volume = {59},
	issn = {00221198},
	url = {http://doi.wiley.com/10.1111/1556-4029.12474},
	doi = {10.1111/1556-4029.12474},
	abstract = {Shoeprints are an important source of information for criminal investigation. Therefore, an increasing number of automatic shoeprint recognition methods have been proposed for detecting the corresponding shoe models. However, comprehensive comparisons among the methods have not previously been made. In this study, an extensive set of methods proposed in the literature was implemented, and their performance was studied in varying conditions. Three datasets of different quality shoeprints were used, and the methods were evaluated also with partial and rotated prints. The results show clear differences between the algorithms: while the best performing method, based on local image descriptors and RANSAC, provides rather good results with most of the experiments, some methods are almost completely unrobust against any unidealities in the images. Finally, the results demonstrate that there is still a need for extensive research to improve the accuracy of automatic recognition of crime scene prints.},
	language = {en},
	number = {6},
	urldate = {2019-04-12},
	journal = {Journal of Forensic Sciences},
	author = {Luostarinen, Tapio and Lehmussola, Antti},
	month = nov,
	year = {2014},
	pages = {1627--1634},
}


@article{davisIntelligenceApproachFootwear1981,
	title = {An {Intelligence} {Approach} to {Footwear} {Marks} and {Toolmarks}},
	volume = {21},
	number = {3},
	journal = {Journal of the Forensic Science Society},
	author = {{R. J. Davis}},
	year = {1981},
	pages = {183--193},
}


@inproceedings{liuWeldDefectImages2018,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Weld {Defect} {Images} {Classification} with {VGG}16-{Based} {Neural} {Network}},
	isbn = {978-981-10-8108-8},
	abstract = {Using X-ray weld defect images for defects detection is a very significant method for non-destructive testing (NDT). Traditionally, this work should be done by skilled technicians who are time-consumed and easily influenced by the environment. Many efforts have been made on automatic classification. However their work either need manual features specified by technicians or get a low accuracy. Some datasets they used for testing are too small to validate the generative capacity. In this paper, we propose a VGG16 based fully convolutional structure to classify the weld defect image, which achieves a high accuracy with a relative small dataset for deep learning method. We choose a dataset with 3000 images for testing the generative capacity of our network, which is large enough compared to others methods. Using this method, we got a 97.6\%97.6\%97.6{\textbackslash}\% test accuracy and 100\%100\%100{\textbackslash}\% train accuracy through our network on two main defects. The time used for each patch is about 0.012 s, which is faster than others methods.},
	language = {en},
	booktitle = {Digital {TV} and {Wireless} {Multimedia} {Communication}},
	publisher = {Springer Singapore},
	author = {Liu, Bin and Zhang, Xiaoyun and Gao, Zhiyong and Chen, Li},
	editor = {Zhai, Guangtao and Zhou, Jun and Yang, Xiaokang},
	year = {2018},
	keywords = {Different size image processing, VGG16-based neural network, Weld defect images},
	pages = {215--223},
	file = {Springer Full Text PDF:/home/srvander/Zotero/storage/2H4NHDFM/Liu et al_2018_Weld Defect Images Classification with VGG16-Based Neural Network.pdf:application/pdf}
}

@inproceedings{oquabLearningTransferringMidlevel2014,
	address = {Columbus, OH, USA},
	title = {Learning and {Transferring} {Mid}-level {Image} {Representations} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4799-5118-5},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909618},
	doi = {10.1109/CVPR.2014.222},
	abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classiﬁcation performance in the largescale visual recognition challenge (ILSVRC2012). The success of CNNs is attributed to their ability to learn rich midlevel image representations as opposed to hand-designed low-level features used in other image classiﬁcation methods. Learning CNNs, however, amounts to estimating millions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be efﬁciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred representation leads to signiﬁcantly improved results for object and action classiﬁcation, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
	language = {en},
	urldate = {2019-04-22},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
	month = jun,
	year = {2014},
	pages = {1717--1724}
}

@article{shinDeepConvolutionalNeural2016,
	title = {Deep {Convolutional} {Neural} {Networks} for {Computer}-{Aided} {Detection}: {CNN} {Architectures}, {Dataset} {Characteristics} and {Transfer} {Learning}},
	volume = {35},
	issn = {0278-0062},
	shorttitle = {Deep {Convolutional} {Neural} {Networks} for {Computer}-{Aided} {Detection}},
	doi = {10.1109/TMI.2016.2528162},
	abstract = {Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.},
	number = {5},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Shin, H. and Roth, H. R. and Gao, M. and Lu, L. and Xu, Z. and Nogues, I. and Yao, J. and Mollura, D. and Summers, R. M.},
	month = may,
	year = {2016},
	keywords = {image analysis, image classification, Databases, Factual, Humans, Image Interpretation, Computer-Assisted, Reproducibility of Results, learning (artificial intelligence), image representation, medical image processing, image recognition, Solid modeling, axial CT slices, Biomedical imaging, CNN architectures, CNN model analysis, Computational modeling, Computed tomography, computer aided diagnosis, computer-aided detection, computer-aided detection problems, computerised tomography, dataset characteristics, deep convolutional neural networks, Diagnosis, Computer-Assisted, diseases, Diseases, fine-tuning CNN models, five-fold cross-validation classification, high performance CAD systems, highly representative hierarchical image features, interstitial lung disease classification, learning data-driven, lung, Lung Diseases, Interstitial, Lungs, Lymph nodes, Lymph Nodes, machine learning, mediastinal LN detection, medical image classification, medical image tasks, medical imaging domain, natural image dataset, neural networks, Neural Networks (Computer), neurophysiology, off-the-shelf pretrained CNN features, pretrained imagenet, reviews, spatial image context, state-of-the-art performance, supervised fine-tuning, thoraco-abdominal lymph node detection, transfer learning, unsupervised CNN pretraining},
	pages = {1285--1298}
}


@incollection{yosinskiHowTransferableAre2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf},
	urldate = {2019-02-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3320--3328}
}


@article{dengImageNetLargeScaleHierarchical,
	title = {{ImageNet}: {A} {Large}-{Scale} {Hierarchical} {Image} {Database}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	language = {en},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	pages = {8}
}


@article{gatysTextureArtDeep2017,
	title = {Texture and art with deep neural networks},
	volume = {46},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095943881730065X},
	doi = {10.1016/j.conb.2017.08.019},
	language = {en},
	urldate = {2019-02-15},
	journal = {Current Opinion in Neurobiology},
	author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
	month = oct,
	year = {2017},
	pages = {178--186}
}

@article{zhongDeepTextUnifiedFramework2016,
	title = {{DeepText}: {A} {Unified} {Framework} for {Text} {Proposal} {Generation} and {Text} {Detection} in {Natural} {Images}},
	shorttitle = {{DeepText}},
	url = {http://arxiv.org/abs/1605.07314},
	abstract = {In this paper, we develop a novel unified framework called DeepText for text region proposal generation and text detection in natural images via a fully convolutional neural network (CNN). First, we propose the inception region proposal network (Inception-RPN) and design a set of text characteristic prior bounding boxes to achieve high word recall with only hundred level candidate proposals. Next, we present a powerful textdetection network that embeds ambiguous text category (ATC) information and multilevel region-of-interest pooling (MLRP) for text and non-text classification and accurate localization. Finally, we apply an iterative bounding box voting scheme to pursue high recall in a complementary manner and introduce a filtering algorithm to retain the most suitable bounding box, while removing redundant inner and outer boxes for each text instance. Our approach achieves an F-measure of 0.83 and 0.85 on the ICDAR 2011 and 2013 robust text detection benchmarks, outperforming previous state-of-the-art results.},
	urldate = {2019-04-22},
	journal = {arXiv:1605.07314 [cs]},
	author = {Zhong, Zhuoyao and Jin, Lianwen and Zhang, Shuye and Feng, Ziyong},
	month = may,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}


@article{labelme,
	title = {{LabelMe}: {A} {Database} and {Web}-{Based} {Tool} for {Image} {Annotation}},
	volume = {77},
	issn = {0920-5691, 1573-1405},
	shorttitle = {{LabelMe}},
	url = {http://link.springer.com/10.1007/s11263-007-0090-8},
	doi = {10.1007/s11263-007-0090-8},
	abstract = {We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative ∗The ﬁrst two authors contributed equally to this work.},
	language = {en},
	number = {1-3},
	urldate = {2019-01-21},
	journal = {International Journal of Computer Vision},
	author = {Russell, Bryan C. and Torralba, Antonio and Murphy, Kevin P. and Freeman, William T.},
	month = may,
	year = {2008},
	pages = {157--173}
}

@book{machineVision,
   title =     {Machine vision},
   author =    {Ramesh Jain and  Rangachar Kasturi and Brian G. Schunck},
   publisher = {McGraw-Hill Science/Engineering/Math},
   isbn =      {0070320187,9780070320185,0071134077,9780071134071,0072384867,9780072384864},
   year =      {1995},
   series =    {},
   edition =   {1},
   volume =    {},
   url =       {http://gen.lib.rus.ec/book/index.php?md5=96A703D27596EB72EE3A6521525EF9CC}
}

@article{ballardGeneralizingHoughTransform1981,
	title = {Generalizing the {Hough} transform to detect arbitrary shapes},
	volume = {13},
	issn = {00313203},
	url = {http://linkinghub.elsevier.com/retrieve/pii/0031320381900091},
	doi = {10.1016/0031-3203(81)90009-1},
	language = {en},
	number = {2},
	urldate = {2019-02-07},
	journal = {Pattern Recognition},
	author = {Ballard, D.H.},
	month = jan,
	year = {1981},

	pages = {111--122},
	file = {Ballard - 1981 - Generalizing the Hough transform to detect arbitra.pdf:/home/srvander/Zotero/storage/DSYITWUH/Ballard - 1981 - Generalizing the Hough transform to detect arbitra.pdf:application/pdf}
}

@book{chollet_allaire_2018,
  title={Deep Learning with R},
  publisher={Manning Publications Company},
  author={Chollet, Francois and Allaire, J.J.},
  year={2018}
}

@misc{smith_2011,
  title={The Forensic Analysis of Footwear Impression Evidence},
  url={https://archives.fbi.gov/archives/about-us/lab/forensic-science-communications/fsc/july2009/review/2009_07_review02.htm},
  journal={FBI},
  publisher={FBI},
  author={Smith, Michael B.},
  year={2011},
  month={Mar}
}

@misc{parent_sandy_significance_nodate,
  title = {Significance of {Class} association of {Shoe} {Prints}},
  url = {https://www.slideserve.com/ayasha/significance-of-class-association-of-shoe-prints},
  author = {{Parent, Sandy}},
  note = {00000}
}

@article{benedict_geographical_2014,
  title = {Geographical variation of shoeprint comparison class correspondences},
  volume = {54},
  number = {5},
  journal = {Science and Justice},
  author = {Benedict, Indrika and Corke, Elisabeth and Morgan-Smith, Rian and Maynard, Philip and Curran, James M and Buckleton, John and Roux, Claude},
  year = {2014},
  note = {00001},
  pages = {335--337},
  file = {Benedict et al_2014_Geographical variation of shoeprint comparison class correspondences.pdf:files/1033/Benedict et al_2014_Geographical variation of shoeprint comparison class correspondences.pdf:application/pdf}
}