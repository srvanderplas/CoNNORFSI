\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Forensic Science International}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

%%% Commenting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[table]{xcolor}
\newcommand{\mt}[1]{{\color{magenta} #1}}
\newcommand{\svp}[1]{{\color{teal} #1}}
\newcommand{\fix}[1]{{\color{orange} #1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<setup, include = F>>=
knitr::opts_chunk$set(echo = F, dev = "pdf", dpi = 1000)
@

\begin{document}

\begin{frontmatter}

\title{A Convolutional Neural Network for Outsole Recognition\tnoteref{t1}}
\tnotetext[t1]{This document is the results of the research
project funded ....}

%% Group authors per affiliation:
% \author{Miranda Tilton and Susan Vanderplas\fnref{csafe}}
% \address{Iowa State University Statistics Department}
% \fntext[csafe]{195 Durham Center, 613 Morill Rd., Ames, IA 50011}

% or include affiliations in footnotes:
\author[1]{Miranda Tilton\fnref{isu}}
\ead{tiltonm@iastate.edu}

\author[1]{Susan Vanderplas\corref{cor1}\fnref{isu}}
\cortext[cor1]{Corresponding author}
\ead{srvander@iastate.edu}

\address[1]{195 Durham Center, 613 Morill Rd, Ames, IA 50010}
\fntext[isu]{Center for Statistical Applications in Forensic Evidence, Iowa State University}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.
\end{abstract}

\begin{keyword}
footwear,class characteristics,computer vision, neural networks
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}
Most forensic examinations of footwear pattern evidence are limited to class characteristic based matches; that is, matches based on the positioning of geometric features within the tread pattern. Some of these features may include brand logos and size markings, while others are more generic geometric shapes, such as circles, triangles, stars, and chevrons. Class characteristic matches can be used for elimination, but are not sufficient for identification, as the characteristics are shared by all shoes with the same size, make, and model \citep{grossVariabilitySignificanceClass2013,bodziakfootwear2000}. Several attempts have been made to automatically identify features in both prints and shoe tread images, with low-level image analysis methods including fractal decomposition \citep{alexanderAutomaticClassificationRecognition1999}, scale-invariant feature recognition \citep{pavlouAutomaticExtractionClassification2006}, Fourier-Mellin transformation \citep{guehamAutomaticRecognitionShoeprints2008}, and other classical image analysis methods. Some of these low-level methods perform relatively well in good conditions \citep{luostarinenMeasuringAccuracyAutomatic2014a} but have degraded performance under conditions which are suboptimal, including those commonly found at crimescenes.

In the wider field of computer vision, attention has turned to more robust methods for image recognition, such as convolutional neural networks (CNNs), which are capable of achieving near-human accuracy under even degraded image conditions\citep{krizhevskyImageNetClassificationDeep2012}. These networks are designed to mimic the process of human vision, and typically involve application of sets of filters, some of which mimic the filters used in the low-level techniques used by early automatic classification attempts. CNNs have been applied in footwear forensics, making use of more general neural networks optimized to detect objects found in natural scenes, such as trees, animals, buildings, and cars; the pre-trained network's features are then used to compare shoes or prints to determine how well they match \citep{kongCrossDomainImageMatching2019a,kongCrossDomainForensicShoeprint2017,zhangAdaptingConvolutionalNeural2017}. Neural network methods appear to be more successful than traditional image analysis techniques, but the features used for matching are not generally informative for humans, who typically classify shoes by patterns and spatial relationships \citep{davisIntelligenceApproachFootwear1981,grossVariabilitySignificanceClass2013}.

In this paper, we discuss an alternate approach that uses an additional model layer to transfer the feature vector output into a vector of probabilities representing the detection of geometric elements in shoe tread images. Working within the feature space used by forensic examiners allows us to augment human-identified features with model output, assessing similarity of different shoe images on a feature set that is explicitly relevant to the domain.

\section{Materials and Methods}
\paragraph{Transfer Learning}
\svp{Neural networks are composed of a set of layers, called the model, base which contain feature detectors, and classifier, the model head, which connect meaningful features to classification labels. A relatively simple convolutional neural network which processes image data may contain more than 14 million parameters in the model base and an additional 120 million parameters in the model head; to train a model of this nature requires millions of labeled images and a significant amount of computational power. Assembling data to train a network from scratch is often a nearly impossible task; the process of \emph{transfer learning} is a natural solution to this common problem. Transfer learning leverages the modularity of neural networks, that is, that the model base can be separated from the classifier which produces predictions.}

\svp{Many convolutional neural networks have been trained on a set of images known as ImageNet; these images are hierarchically labeled, so the same image of a dog may have labels of sky, grass, and trees, and the dog may be labeled as ``mammal'', ``dog'', ``retriever'', and ``golden retriever'' at increasing levels of label complexity\citep{dengImageNetLargeScaleHierarchical}. CNNs trained on ImageNet are optimized for general human-like vision, that is, the ability to recognize a large set of different features simultaneously. As a result, the base of networks trained on these sets are often used for transfer learning, because the initial layers are broadly generalizable to a wide variety of more specific image labeling tasks.} \fix{add a few VGG16 filters here to demonstrate the generic filters in layers 1-4}

\svp{Transfer learning in neural network contexts involves freezing the weights of the pre-trained model base taken from a network trained on a more general set of images, so that during the training process, only the weights of the newly-added classifier are updated\citep{oquabLearningTransferringMidlevel2014}.} In many cases, the entire model base is used, and fitted with a new model head; the modularity of CNNs makes this process relatively simple, and allows researchers to leverage pre-trained networks when working with diffierent sets of image data. Transfer learning allows CNNs to be applied to smaller datasets of several thousand images, reduces the amount of computational time required to fit the model, and provides boosted performance compared to training a new model from scratch \citep{yosinskiHowTransferableAre2014}. It has been successfully applied in automatic classification of medical images \citep{shinDeepConvolutionalNeural2016} as well as in various applications of shoe forensics \citep{kongCrossDomainForensicShoeprint2017,kongCrossDomainImageMatching2019a,zhangAdaptingConvolutionalNeural2017}.

\svp{In this paper, we will demonstrate the use of transfer learning to automatically identify features in shoe treads which are used by forensic examiners, using a general-purpose CNN with relatively simple structure, VGG16, and a database of labeled shoe tread images we have assembled for this project, which are both described below. This approach differs from other approaches \citep{kongCrossDomainForensicShoeprint2017,kongCrossDomainImageMatching2019a,zhangAdaptingConvolutionalNeural2017} in automatic footwear identification which use the output from the model base directly and do not attempt to add human-friendly contextual information with an additional classifier.}

\paragraph{VGG16}
Developed by Oxford's Visual Graphics Group, VGG16 is a CNN with 16 ``functional" (i.e., convolutional and densely connected) layers and 5 ``structural" max pooling layers\citep{krizhevskyImageNetClassificationDeep2012}. In contrast to other popular networks, like ResNet, VGG has a relatively simple structure that provides easier training and interpretability with very little sacrificed accuracy. The simplicity of this structure provides the ability to peer into the inner workings of the network for diagnostic purposes, providing a distinct advantage over more complicated network structures with slightly higher accuracy ratings. VGG16 is a common choice for transfer learning because of this structural simplicity\svp{; it has been used for detection of text in natural images \citep{zhongDeepTextUnifiedFramework2016}, medical imaging classification \citep{oquabLearningTransferringMidlevel2014}, classification of weld defects \citep{liuWeldDefectImages2018}, and many other domain-specific image recognition tasks that are more specific than the ImageNet data on which it was trained. \autoref{fig:VGG16-structure} shows the architecture of VGG16; with transfer learning, the VGG16 model head is replaced with a trained model head specific to recognition of shoeprint class characteristics. }

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/vgg16-shoe-nolabel}
\caption[VGG16 structure]{VGG16 consists of five convolutional blocks that make up the model base. Each convolutional block contains an increasing number of increasingly complex features. After the convolutional blocks, the fully connected layers of the model head are used to make global connections between separate features.}\label{fig:VGG16-structure}
\end{figure}

\svp{In order to train a new head for the VGG16 model base, human-labeled images are necessary. The next section describes how these images were produced and used to fit our custom classifier. }

\paragraph{Annotated Training Data}

\svp{Start with classification scheme. Mention that it was developed in consultation with experts as well as based on past papers and classification schemes.}

\fix{Add classification images}

\svp{Discuss LabelMe and the annotation process. Multiple students working over the course of 6 months... etc.}

\svp{It might be good to add in some (new) statistics relating brand information to common geometric features in tread - show, for instance, the top N brands with bowties vs. the top N brands with polygons. }


Nine geometric class characteristics were chosen: Circle, ..., and Text. Thousands of outsole images were scraped online shoe retail sites. These images were then uploaded for use in a tool called LabelMe (Russell et al., 2008), a labeling/annotating interface which allows users to easily select and label regions of an image. To date, 4371 shoes have been labeled, yielding 27135 multi-label images.



\section{Results}
\paragraph{Overall Accuracy} % Model-wise and class-by-class, w/ confusion matrix

\paragraph{Model Diagnostics}

\section{Conclusions}

\section{Future Work}


\section*{References}

\bibliography{mybibfile}

\end{document}