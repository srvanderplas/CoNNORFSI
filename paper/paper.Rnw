\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Forensic Science International}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

%%% Commenting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[table]{xcolor}
\newcommand{\mt}[1]{{\color{magenta} #1}}
\newcommand{\svp}[1]{{\color{teal} #1}}
\newcommand{\fix}[1]{{\color{orange} #1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<setup, include = F>>=
knitr::opts_chunk$set(echo = F, dev = "pdf", dpi = 1000, message = F, warning = F, fig.align = "center")

library(tidyverse)
library(magrittr)
library(tidyverse)
library(ggplot2)
library(keras)

mytheme <- theme_bw()
theme_set(mytheme)
@

<<results_setup, include = F>>=
if ("paper" %in% list.files()) {
  wd <- file.path(getwd(), "paper")
} else {
  wd <- getwd()
}

codedir <- file.path(wd, "code")
modeldir <- file.path(wd, "model")

source(file.path(codedir, "Generate_Model_Images.R"))
@

<<labeled-data-setup, include = F>>=
model_path <- file.path(modeldir, "TrainedModels")

newest_model <- get_newest(dir = model_path, pattern = "weights.h5")
newest_data_file <- file.path(modeldir, "RProcessedImages",
                              newest_model$process_dir, "cropped_photos.Rdata")

load(newest_data_file)
source(file.path(codedir, "count_images.R"))
@

<<load-keras-model, include = F, eval = T, echo = F>>=
model_dir <- newest_model$path
load(list.files(model_dir, pattern = "-history.Rdata", full.names = T)[1])
load(file.path(get_newest()$path, get_newest(pattern = "\\d.Rdata")$base_file))

model_wts_file <- file.path(newest_model$path, newest_model$base_file)
loaded_model <- set_weights(model_wts_file)
@


\begin{document}

\begin{frontmatter}

\title{A Convolutional Neural Network for Outsole Recognition\tnoteref{t1}}
\tnotetext[t1]{This document is the results of the research
project funded ....}

%% Group authors per affiliation:
% \author{Miranda Tilton and Susan Vanderplas\fnref{csafe}}
% \address{Iowa State University Statistics Department}
% \fntext[csafe]{195 Durham Center, 613 Morill Rd., Ames, IA 50011}

% or include affiliations in footnotes:
\author[1]{Miranda Tilton\fnref{isu}}
\ead{tiltonm@iastate.edu}

\author[1]{Susan Vanderplas\corref{cor1}\fnref{isu}}
\cortext[cor1]{Corresponding author}
\ead{srvander@iastate.edu}

\address[1]{195 Durham Center, 613 Morill Rd, Ames, IA 50011}
\fntext[isu]{Center for Statistical Applications in Forensic Evidence, Iowa State University}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.
\end{abstract}

\begin{keyword}
footwear, class characteristics, computer vision, neural networks
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}
Most forensic examinations of footwear pattern evidence are limited to class characteristic based matches; that is, matches based on the positioning of geometric features within the tread pattern. Some of these features may include brand logos and size markings, while others are more generic geometric shapes, such as circles, triangles, stars, and chevrons. Class characteristic matches can be used for elimination, but are not sufficient for identification, as the characteristics are shared by all shoes with the same size, make, and model \citep{grossVariabilitySignificanceClass2013,bodziakfootwear2000}. Several attempts have been made to automatically identify features in both prints and shoe tread images, with low-level image analysis methods including fractal decomposition \citep{alexanderAutomaticClassificationRecognition1999}, scale-invariant feature recognition \citep{pavlouAutomaticExtractionClassification2006}, Fourier-Mellin transformation \citep{guehamAutomaticRecognitionShoeprints2008}, and other classical image analysis methods. Some of these low-level methods perform relatively well in good conditions \citep{luostarinenMeasuringAccuracyAutomatic2014a} but have degraded performance under conditions which are suboptimal, including those commonly found at crimescenes.

In the wider field of computer vision, attention has turned to more robust methods for image recognition, such as convolutional neural networks (CNNs), which are capable of achieving near-human accuracy under even degraded image conditions \citep{krizhevskyImageNetClassificationDeep2012}. These networks are designed to mimic the process of human vision, and typically involve application of sets of filters, some of which mimic the filters used in the low-level techniques used by early automatic classification attempts. CNNs have been applied in footwear forensics, making use of more general neural networks optimized to detect objects found in natural scenes, such as trees, animals, buildings, and cars; the pre-trained network's features are then used to compare shoes or prints to determine how well they match \citep{kongCrossDomainImageMatching2019a,kongCrossDomainForensicShoeprint2017,zhangAdaptingConvolutionalNeural2017}. Neural network methods appear to be more successful than traditional image analysis techniques, but the features used for matching are not generally informative for humans, who typically classify shoes by patterns and spatial relationships \citep{davisIntelligenceApproachFootwear1981,grossVariabilitySignificanceClass2013}.

In this paper, we discuss an alternate approach that uses an additional model layer to transfer the feature vector output into a vector of probabilities representing the detection of geometric elements in shoe tread images. Working within the feature space used by forensic examiners allows us to augment human-identified features with model output, assessing similarity of different shoe images on a feature set that is explicitly relevant to the domain.
%\svp{Add 1-2 sentences connecting feature sets to the goal of reducing workload - possibly cite solemate as an example of manual feature identification}
In addition, much of the existing software to assess geometric similarity, such as SoleMate\footnote{\url{http://www.fosterfreeman.com/trace-evidence/357-sicar-6-solemate-2.html}}, requires examiners to identify and label features manually; automating classification within the currently used feature space could reduce or eliminate the need for such manual classification.

\section{Materials and Methods}
%\svp{Short bit (1-2 sentences) about low-level feature detectors and why they fail with this problem.}
\mt{Shoeprint evidence from crime scenes is most commonly collected in the form of a photograph, so any useful method to automatically identify outsole characteristics must take the form of an image analysis task.}\svp{We're not working with prints, though - please try to avoid the temptation to delve too far into prints. "Ultimately, most pattern evidence is stored in photographic form; as a result, we must use image analysis methods to create numerical features from the original visual record." or something to that effect.}
There are a number of methods that may be employed to identify shapes and features in an image, such as \svp{Fourier transforms, }Hough transformations \citep{ballardGeneralizingHoughTransform1981} and other low-level feature extraction methods aimed at detecting specific shapes, such as edges, corners, blobs, or ridges \citep[Ch 15]{machineVision}. While these methods are useful in identifying these specific features at a low level, they can be computationally intensive and only identify features on a very small scale; as a result, they cannot reliably identify large geometric shapes like those that may be found in an outsole image.
%\svp{Motivate the use of CNNs directly - complex model, higher level of feature aggregation. Mention pre-trained networks to transition.}
Convolutional neural networks (CNNs) are widely recognized as superior for novel image classification. CNNs are a form of artificial neural network which make use of the image convolution operator used by many low-level feature extraction methods, with the additional ability to aggregate such features and meaningfully connect them to a pre-determined set of labels. CNNs have deep architectures that can be trained to identify complex patterns, but they are structurally similar to the \svp{architecture of the }human visual \svp{system} and output binary or probabilistic predictions for given labels that are readily interpretable. As CNNs make use of labeled training data, the predictions generated are for features which are similar to those identified by humans, resulting in models with greater face validity. Once a CNN is trained, it is relatively fast and easy to apply the model to new images and obtain classifications.

\paragraph{Transfer Learning}
Neural networks are composed of sets of layers; the early layers, which are called the model base, contain feature detectors, and the final layers make up the classifier, or model head, which connects meaningful features to classification labels. A relatively simple convolutional neural network which processes image data may contain more than 14 million parameters in the model base and an additional 120 million parameters in the model head; to train a model of this nature requires millions of labeled images and a significant amount of computational power. Assembling data to train a network from scratch is often a nearly impossible task; the process of \emph{transfer learning} is a natural solution to this common problem. Transfer learning leverages the modularity of neural networks, that is, that the model base can be separated from the classifier which produces predictions.

Many convolutional neural networks have been trained on a set of images known as ImageNet; these images are hierarchically labeled, so the same image of a dog may have labels of sky, grass, and trees, and the dog may be labeled as ``mammal'', ``dog'', ``retriever'', and ``golden retriever'' at increasing levels of label complexity \citep{dengImageNetLargeScaleHierarchical}. CNNs trained on ImageNet are optimized for general human-like vision, that is, the ability to recognize a large set of different features simultaneously. As a result, the base of networks trained on these sets are often used for transfer learning, because the initial layers are broadly generalizable to a wide variety of more specific image labeling tasks. \svp{You need to talk about the model structure of vgg16 before you start talking about layers. Talk of convolutional blocks doesn't really make sense until you show the layer structure.}\autoref{fig:vgg16layers} shows a selection of filters from VGG16, where each filter detects the presence of specific shapes and colors. Each row of filters in \autoref{fig:vgg16layers} corresponds to one of the five convolutional blocks of VGG16. Filters in earlier convolutional blocks detect simple features, such as colors and blobs, while later filters detect more complex combinations of features. Each of the five convolutional blocks has between 128 and 1,536 filters, and a single image is made up of some combination of the thousands of features that are detected by those filters.

Each layer in the model structure is comprised of sets of weights and connections that have been optimized during training. Transfer learning in neural network contexts involves freezing the weights of the pre-trained model base taken from a network trained on a more general set of images, so that during the training process, only the weights of the newly-added classifier are updated \citep{oquabLearningTransferringMidlevel2014}. In many cases, the entire model base is used, and fitted with a new model head; the modularity of CNNs makes this process relatively simple, and allows researchers to leverage pre-trained networks when working with diffierent sets of image data. Transfer learning allows CNNs to be applied to smaller datasets of several thousand images, reduces the amount of computational time required to fit the model, and provides boosted performance compared to training a new model from scratch \citep{yosinskiHowTransferableAre2014}. It has been successfully applied in automatic classification of medical images \citep{shinDeepConvolutionalNeural2016} as well as in various applications of shoe forensics \citep{kongCrossDomainForensicShoeprint2017,kongCrossDomainImageMatching2019a,zhangAdaptingConvolutionalNeural2017}.

\fix{In this paper, we will use transfer learning to automatically identify features in shoe treads which are used by forensic examiners, leveraging a general-purpose CNN with relatively simple structure, VGG16, and a database of labeled shoe tread images we have assembled for this project, which are both described below.} \svp{This sentence is a bit awkward, because you're introducing too many things. The paragraph is good, but I don't think that it makes sense to have it here. Introduce VGG16, then use this paragraph as a transition to talking about the actual model we fit.} This approach differs from other approaches \citep{kongCrossDomainForensicShoeprint2017,kongCrossDomainImageMatching2019a,zhangAdaptingConvolutionalNeural2017} in automatic footwear identification which use the output from the model base directly and do not attempt to add human-friendly contextual information with an additional classifier.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/vgg16-shoe-nolabel}
\caption[VGG16 structure]{VGG16 consists of five convolutional blocks that make up the model base. Each convolutional block contains an increasing number of increasingly complex features. After the convolutional blocks, the fully connected layers of the model head are used to make global connections between separate features.}\label{fig:VGG16-structure}
\end{figure}

\paragraph{VGG16}
Developed by Oxford's Visual Graphics Group, VGG16 is a CNN with 16 ``functional" (i.e., convolutional and densely connected) layers and 5 ``structural" max pooling layers \citep{krizhevskyImageNetClassificationDeep2012}. In contrast to other popular networks, like ResNet, VGG has a relatively simple structure that provides easier training and interpretability with very little sacrificed accuracy. The simplicity of this structure provides the ability to peer into the inner workings of the network for diagnostic purposes, providing a distinct advantage over more complicated network structures with slightly higher accuracy ratings. VGG16 is a common choice for transfer learning because of this structural simplicity; it has been used for detection of text in natural images \citep{zhongDeepTextUnifiedFramework2016}, medical imaging classification \citep{oquabLearningTransferringMidlevel2014}, classification of weld defects \citep{liuWeldDefectImages2018}, and many other domain-specific image recognition tasks that are more specific than the ImageNet data on which it was trained. \autoref{fig:VGG16-structure} shows the architecture of VGG16; with transfer learning, the VGG16 model head is replaced with a trained model head specific to recognition of shoeprint class characteristics.

<<vgg16layers, fig.width = 7, fig.height = 5.75, fig.cap = "A selection of filters from the convolutional layers of VGG16.\\svp{Label the rows with block numbers please}">>=
imgs <- list.files("./images/vgg16layers/", recursive = T, full.names = T)
pngs <- lapply(imgs, function(x) png::readPNG(x)[46:1596, 32:1582, ])
grobs <- lapply(pngs, grid::rasterGrob)
gridExtra::grid.arrange(grobs = grobs, nrow = 5)
@

In order to train a new head for the VGG16 model base, human-labeled images are necessary. The next section describes how these images were produced and used to fit our custom classifier.

%\svp{Moving towards ... how did we train the model}
\paragraph{Annotated Training Data}

Class characteristics are characteristics which can be used to exclude shoes from a match at a crime scene, but cannot be used for individualized matching because they are shared by many shoes (e.g., all shoes of a specific brand and size). A sufficiently well-defined set of features can be used to separate shoes into make and model categories \citep{grossVariabilitySignificanceClass2013}; the set of features used in that study included circle/oval, crepe, herringbone, hexagon, parallel lines, logo/lettering/numbering, perimeter lugs, star, and other. After consulting with practitioners, we developed a set of categories suitable for automatic recognition by convolutional neural networks. These modifications were necessary because some of the definitions used in \citet{grossVariabilitySignificanceClass2013} require contextual spatial information which is not preserved during labeling (for example, lugs are required to be on the perimeter of the shoe). \autoref{tab:class-char-examples} shows three examples of each class.

The categories we use in this study are operationally defined as follows:
\begin{description}
\item [Bowtie] Bowtie shapes are roughly quadrilateral, with two opposite concave faces. The remaining two faces can be convex or straight, and the concave faces may have straight portions, so long as there is a concave region. %Using this definition, shapes such as butterflies are included as bowties. \svp{I'm still on the fence - we can just not mention this, or we could move butterflies to "star"...} \mt{I think it makes a lot of sense to move them to star. That means redoing the poster/slides examples though... grrrrrr} I'm just commenting that bit out for now.
\item [Chevron] Chevron shapes include repeating parallel lines as well as individual ``v" shapes. They may be angular but can also be curved.
\item [Circle] Circles include ellipses and ovals; they must be round.
\item [Line] Lines are repeated and parallel; a more general definition of a line would be difficult to differentiate from many other patterns. Lines can be mildly curved.
\item [Polygon] Polygons are defined in this standard to have more than 4 sides. They include pentagons, hexagons, and octagons.
\item [Quadrilateral] Quadrilaterals (quads) have four sides. They may have rounded or square corners.
\item [Star] Stars are any shape with alternating concave and convex regions, or lines which emanate from a central point. ``X" and ``+" shapes are also classified as stars.
\item [Text] Text is any shape which would be identified as text by a reasonable human. In most cases, the text on the outsole images used is made up of Latin alphabet characters; the model will likely not recognize text in other scripts (but could be trained if non-Latin text images could be obtained).
\item [Triangle] Triangles are any three-sided figure. Like quadrilaterals, they can have rounded corners. In some cases, it is difficult to distinguish between a trapezoidal shape and a triangle when rounded corners are involved.
\item [Other] Other features which were marked include logos, various textures (including crepe, stippling, etc.), and smooth regions with no discernible features. These regions are grouped and provide additional information - that none of the previous nine categories are present.
\end{description}

\begin{table}
\centering
\setlength\tabcolsep{1mm}
\begin{tabular}{rccl}
     Bowtie & \raisebox{-.5\height}{\includegraphics[width=.3\textwidth]{images/class_examples/bowtie_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=.3\textwidth]{images/class_examples/chevron_examples.png}} & Chevron \vspace{1mm}\\
     Circle & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/circle_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/line_examples.png}} & Line  \vspace{1mm}\\
     Polygon & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/polygon_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/quad_examples.png}} & Quad  \vspace{1mm}\\
     Star & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/star_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/text_examples.png}} & Text  \vspace{1mm}\\
     Triangle & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/triangle_examples.png}} &
      \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/other_examples.png}} & Other \\
\end{tabular}
\caption[Geometric elements used to classify tread patterns.]{A set of geometric elements used to classify tread patterns. Categories modified from \cite{grossVariabilitySignificanceClass2013}.}\label{tab:class-char-examples}
\end{table}

Thousands of outsole images were scraped from online shoe retail sites and annotated using LabelMe, a tool for image annotation in computer vision problems \citep{labelme}. After annotation, images are processed by an R script that identifies the minimum bounding rectangle of the region, crops the image to that region, and scales the cropped area to a 256 x 256 pixel image suitable for analysis by the convolutional neural network. During this process, aspect ratio is not preserved, though efforts are made to label regions which are relatively square to minimize the effect of this distortion. To date, \Sexpr{unique(ann_df$base_image) %>% length()} shoes have been labeled, yielding \Sexpr{nrow(dfunion)} multi-label images.

\begin{figure}[hbt]
\centering
  \includegraphics[width=.9\textwidth]{images/LabelMe2.png}
  \caption{An example of labeling images with LabelMe}
\end{figure}

<<class-characteristic-barchart, fig.width = 6, fig.height = 4, dpi = 300, fig.cap = "Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively rare.", fig.scap = "Distribution of classes in all labeled images.">>=
annotated_imgs %>%
  mutate(label_type = str_replace(label_type, "multi", "multiple")) %>%
  mutate(labels = str_to_title(labels)) %>%
  mutate(labels = factor(labels, levels = c("Quad", "Line", "Text", "Circle",
                                            "Chevron", "Triangle", "Polygon",
                                            "Star", "Bowtie", "Other"))) %>%
  ggplot() +
  geom_bar(aes(x = labels, fill = label_type), color = "black") +
  scale_fill_manual("Labels", values = c("single" = "#6ba2b9",
                                         "multiple" = "#2e5597")) +
  coord_flip() +
  ylab("# Labeled Images") +
  xlab("")  +
  ggtitle("Current Class Distribution (All Labeled Images)") +
  mytheme +
  theme(legend.position = c(1, 1),
        legend.justification = c(1.03, 1.03),
        legend.background = element_rect(fill = "white"))
@

<<brand-plots, device = "cairo", include = F>>=
img_names <- dfunion$filename %>% basename() %>% as.vector

labs <- sapply(classes, grepl, x = str_extract(img_names, "^.*?-\\d?-"))
str <- gsub("^.*?-\\d{0,2}-", "", img_names)

# Extract first, first two, and first three words of string
brand_1 <- str_extract(str, "^.*?-")
brand_2 <- str_extract(str, "^.*?-.*?-")
brand_3 <- str_extract(str, "^.*?-.*?-.*?-")

# These are the beginning of brands that need an extra word extracted
multi_brands <- c("the", "the-north", "polo", "polo-ralph", "5", "5-11",
                  "1", "to", "dr", "la", "new", "old", "under",
                  "steve", "bernie", "cole", "tory",
                  "harley", "kristin", "eric",
                  "spring", "chinese", "dirty")

brand <- ifelse(brand_1 %in% paste0(multi_brands, "-"),
                ifelse(brand_2 %in% paste0(multi_brands, "-"),
                       brand_3, brand_2),
                brand_1) %>%
  gsub(. , pattern = "-", replacement = " ") %>%
  trimws(., "right"); rm(brand_1, brand_2, brand_3, multi_brands)

df_images <- cbind(tibble(str, brand), labs)

df_shoes <- df_images %>%
  group_by(str, brand) %>%
  summarize_at(vars(bowtie:triangle), any)

top_brands <- df_shoes %>%
  group_by(brand) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

df_summ <- filter(df_shoes, brand %in% head(top_brands$brand, 8)) %>%
  group_by(brand) %>%
  summarize(n = n(),
            bowtie = sum(bowtie)/n, chevron = sum(chevron)/n,
            circle = sum(circle)/n, line = sum(line)/n,
            polygon = sum(polygon)/n, quad = sum(quad)/n,
            star = sum(star)/n, text = sum(text)/n, triangle = sum(triangle)/n)

df_summ_long <- df_summ %>%
  gather(key = shape, value = prop, -brand, -n) %>%
  arrange(brand)

df_summ_long$brand <- as.factor(df_summ_long$brand)
df_summ_long$shape <- factor(df_summ_long$shape, levels = classes)

# # Line plot
# ggplot(df_summ_long) + geom_point(aes(x = prop, y = shape, color = brand)) +
#   geom_path(aes(x = prop, y = shape, group = brand, color = brand)) +
#   ggtitle("Shape proportions by brand")

# unicode_symbols <- c(bowtie = "\u26FB", chevron = "\uFE3D",
#                      circle =  "\u23FA", line = "\u2225",
#                      polygon = "\u2B23", quad = "\u25A0",
#                      star = "\u2605", text = "a", triangle = "\u25BC",
#                      other = "\u003F")

ggplot(df_summ_long, aes(x = brand, y = prop, shape = factor(shape),
                         color = factor(shape))) +
  geom_point(size = 4) +
  scale_shape_manual(values = c("\u26FB","\uFE3D","\u23FA","\u2225",
                                "\u2B23","\u25A0","\u2605","a","\u25BC")) +
  theme_bw() +
  theme(legend.position = "none") +
  coord_flip() +
  ggtitle("Proportion of labeled shoes containing each shape by brand")
@

<<brands-within-shapes>>=
shapes <- lapply(X = classes, function(x){
  out <- df_shoes[df_shoes[,x, drop = T],]
  out$shape <- x
  out
})
names(shapes) <- classes

shapes %>% map_dfr(., function(x) {
  x %>% group_by(brand) %>%
    summarize(n = n()) %>%
    arrange(desc(n)) %>%
    mutate(prop = n / sum(n),
           shape = x$shape[1:length(n)]) %>%
    head(10)
  }) %>%
  ggplot() +
  geom_text(aes(x = as.factor(shape),
                               y = prop, label = brand,
                               color = as.factor(shape))) +
  theme(legend.position = "none")
@


%Labeled images are scarce relative to the amount of data necessary to train a neural network.
\mt{Although transfer learning substantially reduces the amount of data required to train a CNN, l}abeled images are \mt{still difficult or time-consuming to generate} %scarce \fix{[difficult to generate]}
relative to the amount of data necessary to train \mt{even a small} neural network \mt{adequately. Another common} solution to data scarcity is to artificially enlarge the data set using a process called image augmentation \citep{krizhevskyImageNetClassificationDeep2012}. Augmentation is the transformation of original input data using image operations such as cropping, zoom, skew, rotation, and color balance modification in order to distort or alter the image while maintaining the essential features corresponding to the label. This process reduces the potential for overfitting the model to the specific set of image data used during the training process, and also increases the amount of data available for training; examples of pre- and post-augmentation images are shown in \autoref{fig:augmented}.

\begin{figure}
\centering
\includegraphics[width=.1\textwidth]{images/augmentation/bowtie(R)-1-ugg-sienna-enamel-blue_product_8726365_color_120557.jpg}\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/aug_bowtie(R)-1-ugg-sienna-enamel-blue_product_8726365_color_120557_0_8344.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/quad-2-nike-kids-downshifter-7-infant-toddler-gunsmoke-sunset-pulse-atmosphere-grey_product_8800875_color_720973.jpg}\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/aug_quad-2-nike-kids-downshifter-7-infant-toddler-gunsmoke-sunset-pulse-atmosphere-grey_product_8800875_color_720973_0_7533.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/quad-2-puma-suede-classic-x-mac-three-port-royale-port-royale_product_9123838_color_251426.jpg}\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/aug_quad-2-puma-suede-classic-x-mac-three-port-royale-port-royale_product_9123838_color_251426_0_5689.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/text-1-bernie-mev-kids-catwalk-little-kid-big-kid-multi-camo_product_9084647_color_87089.jpg}\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/aug_text-1-bernie-mev-kids-catwalk-little-kid-big-kid-multi-camo_product_9084647_color_87089_0_1891.jpg}
\caption[Original and augmented images.]{Four sets of original (left) and augmented (right) labeled images.}\label{fig:augmented}
\end{figure}

Model training is analogous to the human learning process. For example, a child learns to identify dogs by being presented with many labeled examples, such as when their parent uses the label ``dog" for an animal walking by. That child's understanding of dogs is then measured by how many dogs the child is able to correctly identify, and also by the number of other animals it mistakenly calls ``dog". Similarly, CNN training is a series of stages, or ``epochs", where the model learns from the set of labeled training data, and that learning is measured through intermediate predictions of validation data.

The \Sexpr{nrow(dfunion)} images were split such that 60\% were used for training. Since \mt{some categories are much more common than others,}
%the categories do not exist in equal proportion in the labeled data,
the training data were weighted by proportion during the training process to prevent the \mt{model learning} from being overwhelmed by more frequent categories. Of the remaining 40\% of data, half were used for validation, to monitor the training process, and the remaining data were for testing the performance of the fitted model.

\svp{This section header might need to move? Haven't decided yet...}
\section{Results}

\subsection{Model Training}
<<training-accuracy, fig.width = 7.5, fig.height = 5, out.width = ".75\\textwidth", fig.cap = "Training and validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 89.5\\% around epoch 9. After that point, validation loss remains the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy.", fig.scap = "Training and validation accuracy and loss during each epoch.", fig.pos = 'h'>>=
data.frame(history$metrics) %>%
  mutate(epoch = 1:n()) %>%
  gather(key = "measure", value = "value", -epoch) %>%
  mutate(Type = ifelse(str_detect(measure, "val"), "Validation", "Training"),
         measure = ifelse(str_detect(measure, "acc"), "Accuracy", "Loss")) %>%
  # bind_rows(tibble(epoch = NA, value =  .6, measure = "Accuracy", Type = "Validation")) %>%
  # bind_rows(tibble(epoch = NA, value =  .33, measure = "Loss", Type = "Validation")) %>%
  ggplot(aes(x = epoch, y = value, color = Type)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_grid(measure~., scales = "free_y", switch = "both") +
  theme_bw() +
  scale_y_continuous("") +
  scale_x_continuous("Epoch") +
  ggtitle("CoNNOR Training Performance") + mytheme +
  theme(axis.title.y = element_blank(), legend.position = c(1, .5),
        legend.justification = c(1.03, -0.05),
        legend.background = element_rect(fill = "white"))
@

\fix{How much should be said about loss and loss functions?}
\mt{A loss function is a function that measures how far model predictions are from the given labels for each image in the validation and test sets; the model ``learns" by updating its predictions in such a way that will minimize this loss.}

\autoref{fig:training-accuracy} shows the training and validation accuracy and loss at each epoch of the fitting process. Overfitting \mt{occurs when a model learns the training data so well that its understanding of the categories becomes specific to only the training cases, which, in turn, leads to poor prediction of new images that were not in the training set.}
%Overfitting, or fitting a model which performs too well on the training data relative to the validation data,
Overfitting is suspected if the validation loss starts to increase after reaching a global minimum. Alternately, underfitting occurs if the validation accuracy is still increasing when model optimization is terminated\mt{, because model performance is still improving with continued training}. Neither of these outcomes appears in \autoref{fig:training-accuracy}, indicating that the model optimization process was halted at an appropriate epoch.

\subsection{Model Accuracy}

\paragraph{Overall Accuracy} % Model-wise and class-by-class, w/ confusion matrix
%\svp{You need to explain these terms. Your CC was written for statisticians, this paper is for practitioners. You need a MUCH gentler introduction to the idea of accuracy, errors, and ROC curves, as well as a lot more definitions for terms.}

\mt{For each image in the test set, the ``true labels" are nine human-assigned labels of 0 or 1 corresponding to each of the nine shape categories. When the CNN predicts which shapes are in the image, however, it assigns a probability between 0 and 1 for each shape category, with a total of nine probabilities per test image. Thus, to determine the accuracy of the model predictions, one must choose a threshold to the probabilities, such that a predicted probability above the threshold indicates a ``yes" prediction from the model and a probability below the threshold indicates a ``no". If the threshold is too high, \fix{moderate} probabilities will be treated as a ``no", which increases the number of true negative and false negative predictions. Conversely, if the threshold is too low, \fix{moderate} probabilites will be considered as ``yes", which increases the true and false positive predictions in the data. An appropriate threshold must be chosen to produce a high true positive rate while also ideally keeping the false positive rate low.}

\mt{Receiver Operating Characteristic (ROC) curves are a type diagnostic plot that compares the false positive rate against the true positive rate for a classification method, computed by comparing the two rates at a number of thresholds, and the Area Under the Curve (AUC) quantifies the shape of the ROC curve. Perfect prediction would be indicated by a right angle along the upper left corner of the plot, with a corresponding AUC of 1, and a diagonal line with an AUC of 0.5 would indicate that the classification method performs no better than random chance.} \autoref{fig:overall-roc} shows the ROC curve for \fix{the model} across all classes, and \autoref{fig:class-roc} shows the curve for each class. The full model has an AUC of 0.88, and the AUC for individual classes ranges from 0.81 (for line) to 0.91 (for bowtie and text). While the class performances do vary slightly, each ROC curve is the same general shape and performs \fix{significantly} better than random chance. \mt{The points in \autoref{fig:class-roc} represent the equal error rate (EER), which is the threshold where there is equal probability of a false positive or false negative error. Thus, for each category, any model prediction with probability above the EER for that shape is considered a positive prediction.}

<<overall-roc, fig.width = 5, fig.height = 5, out.width = ".5\\textwidth", fig.cap = "ROC curve showing overall model performance.", fig.scap = "Overall model performance ROC curve.">>=
library(pROC)
pred_df <- as_tibble(preds) %>% gather(key = feature, value = value)
test_labs_df <- as_tibble(test_labs) %>% gather(key = feature, value = value)
whole_model_roc <- roc(test_labs_df$value, pred_df$value)

whole_model_roc_df <- tibble(tpr = whole_model_roc$sensitivities,
                             fpr = 1 - whole_model_roc$specificities,
                             thresholds = whole_model_roc$thresholds,
                             auc = whole_model_roc$auc[1]) %>%
  nest(tpr, fpr, thresholds, .key =  "roc_plot") %>%
  mutate(eer = purrr::map(roc_plot, eer))
ggplot() +
  geom_line(aes(x = fpr, y = tpr), data = unnest(whole_model_roc_df, roc_plot), size = 1.25) +
  geom_label(aes(x = 1, y = .07, label = sprintf("AUC: %0.2f", auc)), hjust = 1, vjust = -0.2, data = whole_model_roc_df) +
  geom_point(aes(x = fpr, y = tpr, color = "Equal Error Rate"), data = unnest(whole_model_roc_df, eer), size = 2) +
  scale_color_manual("", values = "black") +
  scale_x_continuous("False Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  scale_y_continuous("True Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  ggtitle("CoNNOR Test Set Performance (All Classes)") +
  coord_fixed() + mytheme +
  theme(legend.position = c(1, 0), legend.justification = c(1.01, -0.01), legend.title = element_blank(), legend.background = element_rect(fill = "white"))

@

<<class-roc, fig.width = 8, fig.height = 6, out.width = "\\textwidth", fig.cap = "Class-by-class ROC curves. AUC is area under the curve, a measure of overall model performance. Equal error rates are marked, indicating the position at which there is equal probability of a false positive or false negative error.", fig.scap = "Class-by-class ROC curves.">>=
aucs <- plot_onehot_roc(preds, test_labs, str_to_title(classes))
thresholds <- purrr::map_dbl(aucs$data$eer, ~.$thresholds)
aucs$data$thresholds <- thresholds

ggplot() +
  geom_line(aes(x = fpr, y = tpr), data = unnest(aucs$data, roc_plot), size = 1.25) +
  geom_label(aes(x = 1, y = 0, label = sprintf("AUC: %0.2f\nEER: %0.2f", auc, thresholds)), hjust = 1, vjust = -0.02, data = aucs$data) +
  geom_point(aes(x = fpr, y = tpr, color = "Equal Error\nRate (EER)"), data = unnest(aucs$data, eer), size = 2.5) +
  scale_color_manual("", values = "black") +
  facet_wrap(~class) +
  scale_x_continuous("False Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  scale_y_continuous("True Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  ggtitle("CoNNOR Test Set Performance") +
  facet_wrap(~class, nrow = 2) +
  coord_fixed() +
  theme(legend.position = c(1, 0), legend.justification = c(1, 0))
@

\svp{Pros and cons of confusion matrix here? Maybe we just don't show the actual proportions on the graph? I'd like to discuss this more in person.}

\mt{ROC curves are useful for summarizing the accuracy of model predictions, but they do not indicate which classes are being assigned the false positive or false negative labels when accuracy is not perfect. A confusion matrix is a helpful diagnostic tool to understand which classes are commonly confused for other classes.} A confusion matrix is a \mt{summary of the relationships between} the labels assigned to an image and the labels predicted by the model.

In the confusion matrix presented in \autoref{fig:confusion_matrix}, the values along the diagonal represent the proportion of true positives captured within each category. The off-diagonal values represent the proportion of false positives for each shape pairing; \mt{however, since a single image may truly contain multiple shapes}, the values are adjusted to remove the effect of any true positives from the calculation of false positive proportions. For example, to calculate the proportion of images that contain triangles but are being falsely labeled as containing quadrilaterals, any image that truly contains both triangles and quadrilaterals is removed before calculating the proportion of false quadrilateral labels.

<<confusion_matrix, fig.width = 9, fig.height = 8, out.width = "\\textwidth", fig.cap = "Confusion matrix, showing on the diagonal the correct classification rate and on the off-diagonal, classification errors. Note that in multi-label images, correct off-diagonal labels have been excluded from the calculation of false positives.", fig.scap = "Confusion matrix, with correct and incorrect model classifications.", dpi = 600>>=
get_confusion_matrix(predictions = preds, classes = classes,
                     test_labels = test_labs, threshold = thresholds) %>%
  set_names(str_to_title(classes)) %>%
  ggcorrplot(., hc.order = F, outline.col = "white", lab = T) +
  scale_fill_gradient("Classification\nRate", low = "white",
                      high = "cornflowerblue", limits = c(0, 1)) +
  scale_x_discrete("Image Label") + scale_y_discrete("Prediction") +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14, angle = 90, vjust = 1)) +
  ggtitle("CoNNOR Multi-Class Confusion Matrix: Test Set Performance") +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "grey50"),
        panel.grid.minor = element_line(color = "grey60")) +
  theme(plot.margin = grid::unit(c(0,0,0,0), "mm"),
        plot.background = element_rect(fill = "transparent", color = NA),
        plot.subtitle = element_blank(), plot.caption = element_blank(),
        panel.spacing = unit(c(0, 0, 0, 0), "mm"))
@

The horizontal band in \autoref{fig:confusion_matrix} indicates that quadrilaterals are predicted more often than they should be for every true label. Similarly, polygons, stars, and triangles produce a large number of false positive predictions into other categories, as evidenced by the vertical bands. Another interesting artifact in \autoref{fig:confusion_matrix} is that circles are often misclassified as text, and text is often mislabeled as containing circles. Not all of the images used in model training are accurately labeled: in some cases, the labels have not \mt{yet} been updated after labeling guidelines have changed. The confusion between text and circles is a natural consequence of the overlap between geometric shapes and components of written text; it is not indicative of a lack of fit as much as a weakness in the classification system used to define the model.

\autoref{fig:confusion_matrix} shows \mt{which classes are being predicted incorrectly, but does not necessarily suggest \emph{why} the confusion is taking place. To investigate further, it is useful to} \fix{see what the model is seeing.}
%the general weaknesses of the model, but does not necessarily suggest where to look as to \emph{why} the model is not performing appropriately.

\paragraph{Model Predictions}

 \mt{What else to say here?}

<<exemplars-consistency, fig.cap = "Examples", fig.pos = "p!", out.width = "100%", cache = T, fig.pos = "!h">>=
exemplar <- list.files(file.path(wd, "images", "consistency", "exemplar"), full.names = T)
pred_prob_plot(exemplar, loaded_model)
@

\fix{Can also look at many examples within same shape categories.}

<<chevrons-consistency, fig.cap = "Chevrons", fig.pos = "p!", out.width = "100%", cache = T>>=
chevron <- list.files(file.path(wd, "images", "consistency", "chevron"), full.names = T)
pred_prob_plot(chevron, loaded_model)
@

<<circle-text-consistency, fig.cap = "Text with circles or circle-like shapes", fig.pos = "p!", out.width = "100%", cache = T>>=
circle_text <- list.files(file.path(wd, "images", "consistency", "circle-text"), full.names = T)
pred_prob_plot(circle_text, loaded_model)
@


\paragraph{Model Diagnostics}

\mt{A heatmap is a visual diagnostic tool that highlights the portions of an image that are most significant to classification for a given class. Using these graphics, we can identify which locations in an image contribute most to the model's predictions.} In \fix{the figures in this subsection,} heatmaps are shown for a selection of three classes, which are either contained in the image or have high output probabilities, alongside the original image.

\fix{I picked the heatmaps I thought illustrated the point best, but didn't modify them heavily yet. How many do we want to include, and do we want to highlight model challenges or just focus on the point that CoNNOR sees what we see?}


<<chevron-circle-heatmap, eval = T, out.width = ".95\\textwidth", fig.width = 8, fig.height = 6, fig.scap = "Heatmaps of an image containing chevrons and circles.", fig.cap = "Chevrons are identified by sharp corners and parallel lines, but the thick line between the two shape sections is falsely identified as a quadrilateral.", fig.pos = "!h">>=
heatmaps <- list.files(file.path(wd, "images", "heatmaps"),
                       pattern = "^heatmap", full.names = T)
knitr::include_graphics(heatmaps[grepl("chevron-circle", heatmaps)])
@

<<text-seychelles-heatmap, eval = T, out.width = ".95\\textwidth", fig.width = 8, fig.height = 6, fig.scap = "Heatmaps of an image containing text.", fig.cap = "This heatmap illustrates that the high probability of a circle is a result of the highly curled tails of the 'S' in the text.">>=
knitr::include_graphics(heatmaps[grepl("text", heatmaps)])
@

<<corks-heatmap, eval = T, out.width = ".95\\textwidth", fig.width = 8, fig.height = 6, fig.scap = "Heatmaps of an image containing none of the target classes.", fig.cap = "This outsole shows repeated outlines of corks, which do not fall into any of the nine classes; the concave corners where the corks overlap are predicted to be stars, and the round edges lead to predictions of circles.">>=
knitr::include_graphics(heatmaps[grepl("corks", heatmaps)])
@


\section{Conclusions}
\svp{Rework this to address 3 main conclusions - 1: NNs work for automatic feature ID, 2: Different levels of feature variability within classes, and 3: correcting for contrast is important.}
The goal of this research was to develop a method to automatically identify geometric class characteristics of shoe outsoles. Such a method takes the ``impossible" problem of quantifying outsole features in a given population and lays the groundwork for a tractable solution. A set of geometric class characteristics was defined to both broadly classify a large variety of shoes and to narrow down similarity into a manageable number and type of features for further use in a shoeprint analysis. The final set of features used in this research achieves both goals quite well, and has potential for much finer-grained analysis when recognized features are combined with spatial information and relationships.

CoNNOR was developed using the convolutional base of VGG16 and a newly trained classifier/head to identify the defined set of geometric features. After training, CoNNOR performs well on the data provided. In general, the model is able to identify many well-defined geometric shapes in the images both consistently and accurately. The prediction accuracy of this model provides the ability to compute statistics for the frequency of given features in a well-defined population.

\section{Future Work}

\svp{Need to show image contrast stuff to use this paragraph.}
Although CoNNOR performs well in its current state, there are still a number of ways to potentially improve prediction accuracy. For one, image contrast  plays a large role in how well the model classifies the geometric shapes present. Thus, exploring methods of color correction, such as histogram normalization, may prove useful for eliminating the effect of contrast on predictions. Geometric features are also relatively simple with respect to the features that are being detected by the final convolutional block of VGG16, as in \autoref{fig:vgg16layers}; it is quite possible that prediction accuracy could improve by directly classifying the features that are output by the fourth block, rather than using the full convolutional base.


\bibliography{mybibfile}

\end{document}