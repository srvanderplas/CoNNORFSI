\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

% Allow strikethrough text
\usepackage[normalem]{ulem}

\journal{Forensic Science International}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

%%% Commenting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[table]{xcolor}
\newcommand{\mt}[1]{{\color{magenta} #1}}
\newcommand{\svp}[1]{{\color{teal} #1}}
\newcommand{\fix}[1]{{\color{orange} #1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<setup, include = F>>=
knitr::opts_chunk$set(echo = F, dev = "pdf", dpi = 1000, message = F, warning = F, fig.align = "center", fig.path = "figure/")

library(tidyverse)
library(magrittr)
library(tidyverse)
library(ggplot2)
library(keras)

mytheme <- theme_bw()
theme_set(mytheme)
@

<<results_setup, include = F>>=
if ("paper" %in% list.files()) {
  wd <- file.path(getwd(), "paper")
} else {
  wd <- getwd()
}

codedir <- file.path(wd, "code")
modeldir <- file.path(wd, "model")
imgdir <- file.path(wd, "images")

source(file.path(codedir, "Generate_Model_Images.R"))
@

<<labeled-data-setup, include = F>>=
model_path <- file.path(modeldir, "TrainedModels")

if(grepl("!School", model_path)) {
  newest_model <- get_newest(dir = model_path, pattern = "weights.h5")
} else {
  newest_model <- list(path = file.path(model_path, "20190514-155453"),
      base_file = "2019-05-14_19:45:22_vgg16_onehotaug_9class_256-weights.h5",
      prefix = "vgg16_onehotaug_9class_256", start_date = "2019-05-14_19:45:22",
      process_dir = "20190514-155453")
}

newest_data_file <- file.path(modeldir, "RProcessedImages",
                              newest_model$process_dir, "cropped_photos.Rdata")

load(newest_data_file)
source(file.path(codedir, "count_images.R"))
@

<<load-keras-model, include = F, eval = T, echo = F>>=
model_dir <- newest_model$path
load(list.files(model_dir, pattern = "-history.Rdata", full.names = T)[1])
load(file.path(get_newest()$path, get_newest(pattern = "\\d.Rdata")$base_file))

model_wts_file <- file.path(newest_model$path, newest_model$base_file)
loaded_model <- set_weights(model_wts_file)
@

\begin{document}

\begin{frontmatter}

\title{A Convolutional Neural Network for Outsole Recognition\tnoteref{t1}}
\tnotetext[t1]{This document is the results of the research
project funded ....}

%% Group authors per affiliation:
% \author{Miranda Tilton and Susan Vanderplas\fnref{csafe}}
% \address{Iowa State University Statistics Department}
% \fntext[csafe]{195 Durham Center, 613 Morill Rd., Ames, IA 50011}

% or include affiliations in footnotes:
\author[1]{Miranda Tilton\fnref{isu}}
\ead{tiltonm@iastate.edu}

\author[1]{Susan Vanderplas\corref{cor1}\fnref{isu}}
\cortext[cor1]{Corresponding author}
\ead{srvander@iastate.edu}

\address[1]{195 Durham Center, 613 Morill Rd, Ames, IA 50011}
\fntext[isu]{Center for Statistical Applications in Forensic Evidence, Iowa State University}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.
\end{abstract}

\begin{keyword}
footwear, class characteristics, computer vision, neural networks
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}
Most forensic examinations of footwear pattern evidence are limited to class characteristic based matches; that is, matches based on the positioning of geometric features within the tread pattern. Some of these features may include brand logos and size markings, while others are more generic geometric shapes, such as circles, triangles, stars, and chevrons. Class characteristics can be used for elimination, but are not sufficient for identification, as the characteristics are shared by all shoes with the same size, make, and model \citep{grossVariabilitySignificanceClass2013,bodziakfootwear2000}. Several attempts have been made to automatically identify features in both prints and shoe tread images, with low-level image analysis methods including fractal decomposition \citep{alexanderAutomaticClassificationRecognition1999}, scale-invariant feature recognition \citep{pavlouAutomaticExtractionClassification2006}, Fourier-Mellin transformation \citep{guehamAutomaticRecognitionShoeprints2008}, and other classical image analysis methods. Some of these low-level methods perform relatively well in good conditions \citep{luostarinenMeasuringAccuracyAutomatic2014a} but have degraded performance under conditions which are suboptimal, including those commonly found at crime scenes.

In the wider field of computer vision, attention has turned to more robust methods for image recognition, such as convolutional neural networks (CNNs), which are capable of achieving near-human accuracy under even degraded image conditions \citep{krizhevskyImageNetClassificationDeep2012}. These networks are designed to mimic the process of human vision, and typically involve application of sets of filters, some of which mimic the filters used in the low-level techniques used in early automatic classification attempts. CNNs have been applied in footwear forensics, making use of more general neural networks optimized to detect objects found in natural scenes, such as trees, animals, buildings, and cars; the pre-trained network's features are then used to compare shoes or prints to determine how well they match \citep{kongCrossDomainImageMatching2019a,kongCrossDomainForensicShoeprint2017,zhangAdaptingConvolutionalNeural2017}. Neural network methods appear to be more successful than traditional image analysis techniques, but the features used for matching are not generally informative for humans, who typically classify shoes by patterns and spatial relationships \citep{davisIntelligenceApproachFootwear1981,grossVariabilitySignificanceClass2013}.

In this paper, we discuss an alternate approach that uses an additional model layer to transfer the feature vector output into a vector of probabilities representing the detection of geometric elements in shoe tread images. Working within the feature space used by forensic examiners allows us to augment human-identified features with model output, assessing similarity of different shoe images on a feature set that is explicitly relevant to the domain. In addition, much of the existing software to assess geometric similarity, such as SoleMate\footnote{\url{http://www.fosterfreeman.com/trace-evidence/357-sicar-6-solemate-2.html}}, requires examiners to identify and label features manually; automating classification within the currently used feature space could reduce or eliminate the need for such manual classification.


\section{Materials and Methods}
%\fix{We may want to mention some of the methods in the introduction for continuity sake...}
%\svp{I was talking about the methods above (e.g. fractal decomp, scale invariant feature recognition, etc., not methods we used)}
Ultimately, most pattern evidence is stored in photographic form; as a result, we must use image analysis methods to create numerical features from the original visual record. There are a number of methods that may be employed to identify shapes and features in an image, such as Fourier\mt{-Mellin} transforms, \mt{fractal decomposition, scale-invariant feature recognition}, Hough transformations \citep{ballardGeneralizingHoughTransform1981} and other low-level feature extraction methods aimed at detecting specific shapes, such as edges, corners, blobs, or ridges \citep[Ch 15]{machineVision}. While these methods are useful in identifying these specific features at a low level, they can be computationally intensive and only identify features on a very small scale; as a result, they cannot reliably identify large geometric shapes like those that may be found in a high resolution outsole image.

Convolutional neural networks (CNNs) are widely recognized as superior for novel image classification and feature detection. CNNs are a form of artificial neural network which make use of the image convolution operator used by many low-level feature extraction methods, with the additional ability to aggregate such features and meaningfully connect them to a pre-determined set of labels. CNNs have deep architectures that can be trained to identify complex patterns, but they are structurally similar to the architecture of the human visual system and output binary or probabilistic predictions for given labels that are readily interpretable. As CNNs make use of labeled training data, the predictions generated are for features which are similar to those identified by humans, resulting in models with greater face validity. Once a CNN is trained, it is relatively fast and easy to apply the model to new images and obtain classifications.

\paragraph{Transfer Learning}
Neural networks are composed of sets of layers; the early layers, which are called the model base, contain feature detectors, and the final layers make up the classifier, or model head, which connects meaningful features to classification labels (see \autoref{fig:VGG16-structure} for an example of the structure of a CNN). Each of these layers is composed of sets of weights and connections that are optimized during training. A relatively simple convolutional neural network which processes image data may contain more than 14 million parameters in the model base and an additional 120 million parameters in the model head; to train a model of this nature requires millions of labeled images and a significant amount of computational power. Assembling sufficient data to train a network from scratch is a gargantuan task; the process of \emph{transfer learning} is a natural solution to this common problem. Transfer learning leverages the modularity of neural networks, that is, that the model base that detects features can be separated from the classifier which produces predictions. Transfer learning approaches use the base of a model trained on more general image data and train a new model head or classifier using a smaller set of domain-specific labeled images\citep{oquabLearningTransferringMidlevel2014}. This allows CNNs to be applied to smaller datasets of several thousand images, reduces the amount of computational time required to fit the model, and provides boosted performance compared to training a new model from scratch \citep{yosinskiHowTransferableAre2014}. Transfer learning has been successfully applied in automatic classification of medical images \citep{shinDeepConvolutionalNeural2016} as well as in various applications of shoe forensics \citep{kongCrossDomainForensicShoeprint2017,kongCrossDomainImageMatching2019a,zhangAdaptingConvolutionalNeural2017}.

Many convolutional neural networks have been trained on a subset of images from an online image database called ImageNet \citep{dengImageNetLargeScaleHierarchical}. In particular, there is a standard set of images from ImageNet, consisting of 1.2 million images \mt{that span} 1,000 catetories, that is commonly used in computer vision tasks. CNNs trained on ImageNet are optimized for general human-like vision, that is, the ability to recognize a large set of different features simultaneously. As a result, the base of networks trained on these sets are often used for transfer learning, because the initial layers are broadly generalizable to a wide variety of more specific image labeling tasks. In the next section, we will introduce the pre-trained network VGG16, pictured in \autoref{fig:VGG16-structure}, and examine a selection of its filters (shown in \autoref{fig:vgg16layers}) that demonstrate the levels of detail in the initial layers of a model trained on generic data.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/vgg16-shoe-nolabel}
\caption[VGG16 structure]{VGG16 consists of five convolutional blocks that make up the model base. Each convolutional block contains an increasing number of increasingly complex features. After the convolutional blocks, the fully connected layers of the model head are used to make global connections between separate features.}\label{fig:VGG16-structure}
\end{figure}

\paragraph{VGG16}
Developed by Oxford's Visual Graphics Group, VGG16 is a CNN trained on ImageNet
\citep{krizhevskyImageNetClassificationDeep2012}. In contrast to other popular pre-trained networks, such as AlexNet or ResNet, VGG has a relatively simple structure that provides easier training and interpretability with very little sacrificed accuracy. The simplicity of this structure provides the ability to peer into the inner workings of the network for diagnostic purposes, providing a distinct advantage over more complicated network structures with slightly higher accuracy ratings. VGG16 is a common choice for transfer learning because of this structural simplicity; it has been used for detection of text in natural images \citep{zhongDeepTextUnifiedFramework2016}, medical imaging classification \citep{oquabLearningTransferringMidlevel2014}, classification of weld defects \citep{liuWeldDefectImages2018}, and many other domain-specific image recognition tasks that are more specific than the ImageNet data on which it was trained. \autoref{fig:VGG16-structure} shows the architecture of VGG16; with transfer learning, the VGG16 model head is replaced with a model head trained specifically to recognize shoeprint class characteristics.

The base of VGG16 is made up of five convolutional blocks, each of which contain between 128 and 1,536 filters. A filter is a set of numerical weights that, when applied to an image, quantifies the presence of the specific shapes and/or colors that the filter is trained to detect. \autoref{fig:vgg16layers} shows a selection of images which maximally activate specific} filters from VGG16, with each row corresponding to one of the five convolutional blocks. Filters in earlier convolutional blocks detect simple features, such as colors, lines, corners, and blobs, while later filters detect more complex combinations of features. This aggregation of features through successive convolutional layers mirrors the process of complex feature detection in human vision. Any complex feature can be disassembled into a set of simple features; similarly, the successive sets of layers detect increasingly detailed features.

Although the filters of VGG16 have been trained and optimized on the ImageNet data, the large number of filters span a wide variety of shapes and features, many of which are useful when examining tread patterns. Just as humans can detect the features of an unfamiliar object (consituent parts, edges, textures, and corners), a CNN trained on a wide set of objects can be used to classify unrelated images, such as shoe soles.

<<vgg16layers, fig.width = 7, fig.height = 5.75, fig.cap = "A selection of filters from the convolutional layers of VGG16.">>=

if(!file.exists(file.path(wd, "images", "unlabeled_filters.png"))) {
  imgs <- list.files(file.path(wd, "images", "vgg16layers"),
                     recursive = T, full.names = T)
  pngs <- lapply(imgs, function(x) png::readPNG(x)[46:1596, 32:1582, ])
  grobs <- lapply(pngs, grid::rasterGrob)
  filters <- gridExtra::arrangeGrob(grobs = grobs, nrow = 5)
  ggsave(filename = file.path(wd, "images", "unlabeled_filters.png"),
         plot = filters, device = "png", width = 6, height = 5.3)
}

if(!file.exists(file.path(wd, "images", "labeled_filters.png"))) {
  library(magick)
  b <- image_blank(width = 300, height = 1590)
  f <- image_read(file.path(wd, "images", "unlabeled_filters.png"))
  f_l <- image_append(c(b, f)) %>%
    image_annotate("Block 1:", location = "+60+139",
                   color = "black", size = 60) %>%
    image_annotate("Block 2:", location = "+60+457",
                   color = "black", size = 60) %>%
    image_annotate("Block 3:", location = "+60+775",
                   color = "black", size = 60) %>%
    image_annotate("Block 4:", location = "+60+1093",
                   color = "black", size = 60) %>%
    image_annotate("Block 5:", location = "+60+1411",
                   color = "black", size = 60) %>%
    image_scale(geometry = "600x")
  image_write(image = f_l, path = file.path(wd, "images", "labeled_filters.png"),
              format = "png")
}

knitr::include_graphics(file.path(wd, "images", "labeled_filters.png"))
@

In this paper, we demonstrate the use of transfer learning to automatically identify features in shoe treads that are used by forensic examiners. To do this, we leverage the relatively simple structure and generalizability of VGG16 and train a new classifier using a database of labeled shoe tread images we have assembled for this project. This approach differs from other approaches \citep{kongCrossDomainForensicShoeprint2017,kongCrossDomainImageMatching2019a,zhangAdaptingConvolutionalNeural2017} in automatic footwear identification that use the output from the model base directly and do not attempt to add human-friendly contextual information with an additional classifier. The next section describes how the database of images was produced and used to fit our custom classifier.

\paragraph{Annotated Training Data}
Existing research indicates that a sufficiently well-defined set of features can be used to separate shoes into make and model categories \citep{grossVariabilitySignificanceClass2013}; the set of features used in that study included circle/oval, crepe, herringbone, hexagon, parallel lines, logo/lettering/numbering, perimeter lugs, star, and other. After consulting with practitioners, we developed a set of categories suitable for automatic recognition by convolutional neural networks. These modifications were necessary because some of the definitions used in Gross et al. (2013) require contextual spatial information which is not preserved during labeling (for example, lugs are required to be on the perimeter of the shoe). \autoref{tab:class-char-examples} shows three examples of each class in \mt{the modified} feature set.


The categories we use in this study are operationally defined as follows:
\begin{description}
\item [Bowtie] Bowtie shapes are roughly quadrilateral, with two opposite concave faces. The remaining two faces can be convex or straight, and the concave faces may have straight portions, so long as there is a concave region.
\item [Chevron] Chevron shapes include repeating parallel lines as well as individual ``v" shapes. They may be angular but can also be curved.
\item [Circle] Circles include ellipses and ovals; they must be round.
\item [Line] Lines are repeated and parallel; a more general definition of a line would be difficult to differentiate from many other patterns. Lines can be mildly curved.
\item [Polygon] Polygons are defined in this standard to have more than 4 sides. They include pentagons, hexagons, and octagons.
\item [Quadrilateral] Quadrilaterals (quads) have four sides. They may have rounded or square corners.
\item [Star] Stars are any shape with alternating concave and convex regions, or lines which emanate from a central point. ``X" and ``+" shapes are also classified as stars.
\item [Text] Text is any shape which would be identified as text by a reasonable human. In most cases, the text on the outsole images used is made up of Latin alphabet characters; the model will likely not recognize text in other scripts (but could be trained if non-Latin text images could be obtained).
\item [Triangle] Triangles are any three-sided figure. Like quadrilaterals, they can have rounded corners. In some cases, it is difficult to distinguish between a trapezoidal shape and a triangle when rounded corners are involved.
\item [Other] Other features which were marked include logos, various textures (including crepe, stippling, etc.), and smooth regions with no discernible features. These regions are grouped and provide the additional information that none of the previous nine categories are present.
\end{description}

\begin{table}
\centering
\setlength\tabcolsep{1mm}
\begin{tabular}{rccl}
Bowtie & \raisebox{-.5\height}{\includegraphics[width=.3\textwidth]{images/class_examples/bowtie_examples.png}} &
\raisebox{-.5\height}{\includegraphics[width=.3\textwidth]{images/class_examples/chevron_examples.png}} & Chevron \vspace{1mm}\\
Circle & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/circle_examples.png}} &
\raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/line_examples.png}} & Line  \vspace{1mm}\\
Polygon & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/polygon_examples.png}} &
\raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/quad_examples.png}} & Quad  \vspace{1mm}\\
Star & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/star_examples.png}} &
\raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/text_examples.png}} & Text  \vspace{1mm}\\
Triangle & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/triangle_examples.png}} &
\raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/class_examples/other_examples.png}} & Other \\
\end{tabular}
\caption[Geometric elements used to classify tread patterns.]{A set of geometric elements used to classify tread patterns. Categories modified from \cite{grossVariabilitySignificanceClass2013}.}\label{tab:class-char-examples}
\end{table}

Thousands of outsole images were obtained from online shoe retail sites and annotated using LabelMe, a tool for image annotation in computer vision problems \citep{labelme}. After annotation, the minimum bounding rectangle of the region is identified and the image is cropped to that area; subsequently, the cropped image is scaled to 256 x 256 pixels. During this process, aspect ratio is not preserved, though efforts are made to label regions which are relatively square to minimize the effect of this distortion. To date, \Sexpr{unique(ann_df$base_image) %>% length()} shoes have been labeled, yielding \Sexpr{nrow(dfunion)} multi-label images.

\begin{figure}[hbt]
\centering
\includegraphics[width=.9\textwidth]{images/LabelMe2.png}
\caption{An example of labeling images with LabelMe}
\end{figure}

<<class-characteristic-barchart, fig.width = 6, fig.height = 4, dpi = 300, out.width = "80%", fig.cap = "Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively rare.", fig.scap = "Distribution of classes in all labeled images.">>=
annotated_imgs %>%
  mutate(label_type = str_replace(label_type, "multi", "multiple")) %>%
  mutate(labels = str_to_title(labels)) %>%
  mutate(labels = factor(labels, levels = c("Quad", "Line", "Text", "Circle",
                                            "Chevron", "Triangle", "Polygon",
                                            "Star", "Bowtie", "Other"))) %>%
  ggplot() +
  geom_bar(aes(x = labels, fill = label_type), color = "black") +
  scale_fill_manual("Labels", values = c("single" = "#6ba2b9",
                                         "multiple" = "#2e5597")) +
  coord_flip() +
  ylab("# Labeled Images") +
  xlab("")  +
  ggtitle("Current Class Distribution (All Labeled Images)") +
  mytheme +
  theme(legend.position = c(1, 1),
        legend.justification = c(1.03, 1.03),
        legend.background = element_rect(fill = "white"))
@

Transfer learning reduces the amount of data required to train a CNN by several orders of magnitude, but labeled images are still difficult to generate on a large scale. Thus, to make the most efficient use of the existing labeled data, we enlarge the training data using a process called image augmentation \citep{krizhevskyImageNetClassificationDeep2012}. Augmentation is the transformation of original input data using image operations such as cropping, zoom, skew, rotation, and color balance modification in order to distort or alter the image while maintaining the essential features corresponding to the label. This process reduces the potential for overfitting the model to the specific set of image data used during the training process, and also increases the amount of data available for training. Examples of pre- and post-augmentation images are shown in \autoref{fig:augmented}.

\begin{figure}
\centering
\includegraphics[width=.1\textwidth]{images/augmentation/bowtie(R)-1-ugg-sienna-enamel-blue_product_8726365_color_120557.jpg}\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/aug_bowtie(R)-1-ugg-sienna-enamel-blue_product_8726365_color_120557_0_8344.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/quad-2-nike-kids-downshifter-7-infant-toddler-gunsmoke-sunset-pulse-atmosphere-grey_product_8800875_color_720973.jpg}\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/aug_quad-2-nike-kids-downshifter-7-infant-toddler-gunsmoke-sunset-pulse-atmosphere-grey_product_8800875_color_720973_0_7533.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/quad-2-puma-suede-classic-x-mac-three-port-royale-port-royale_product_9123838_color_251426.jpg}\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/aug_quad-2-puma-suede-classic-x-mac-three-port-royale-port-royale_product_9123838_color_251426_0_5689.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/text-1-bernie-mev-kids-catwalk-little-kid-big-kid-multi-camo_product_9084647_color_87089.jpg}\hfill
\includegraphics[width=.1\textwidth]{images/augmentation/aug_text-1-bernie-mev-kids-catwalk-little-kid-big-kid-multi-camo_product_9084647_color_87089_0_1891.jpg}
\caption[Original and augmented images.]{Four sets of original (left) and augmented (right) labeled images.}\label{fig:augmented}
\end{figure}

Model training is analogous to the human learning process. For example, a child learns to identify dogs by being presented with many labeled examples, such as when their parent uses the label ``dog" for an animal walking by. That child's understanding of dogs is then measured by how many dogs the child is able to correctly identify, and also by the number of other animals it mistakenly calls ``dog". Similarly, CNN training is a series of stages, or ``epochs", where the model learns from the set of labeled training data, and that learning is measured through intermediate predictions on validation data.

The \Sexpr{nrow(dfunion)} images were split such that 60\% were used for training. Since the categories do not exist in equal proportion in the labeled data,
the training data were weighted by proportion during the training process to ensure that the model can identify both rare and common geometric shapes. Of the remaining 40\% of data, half were used for validation, to monitor the training process, and the remaining data were for testing the performance of the final model.

\section{Results}

\subsection{Model Training}
During model fitting, the model ``learns" from the training set and is evaluated on the validation set, which consists of previously unseen images (i.e. images not used during the training process). Learning occurs by minimization of the ``loss", which is a function of the distance between the model predictions and the image labels. As the loss decreases, the model's accuracy (the proportion of correct labels) increases. During model training, we expect that the accuracy for the training set will increase beyond the accuracy for the validation set; the validation accuracy will eventually level off. Similarly, we expect the loss for the training set and the validation set to decrease initially. \autoref{fig:training-accuracy} shows the training and validation accuracy and loss at each epoch of the fitting process.

<<training-accuracy, fig.width = 7.5, fig.height = 5, out.width = ".75\\textwidth", fig.cap = "Training and validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 89.5\\% around epoch 9. After that point, validation loss remains the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy.", fig.scap = "Training and validation accuracy and loss during each epoch.", fig.pos = 'h'>>=
data.frame(history$metrics) %>%
  mutate(epoch = 1:n()) %>%
  gather(key = "measure", value = "value", -epoch) %>%
  mutate(Type = ifelse(str_detect(measure, "val"), "Validation", "Training"),
         measure = ifelse(str_detect(measure, "acc"), "Accuracy", "Loss")) %>%
  # bind_rows(tibble(epoch = NA, value =  .6, measure = "Accuracy", Type = "Validation")) %>%
  # bind_rows(tibble(epoch = NA, value =  .33, measure = "Loss", Type = "Validation")) %>%
  ggplot(aes(x = epoch, y = value, color = Type)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_grid(measure~., scales = "free_y", switch = "both") +
  theme_bw() +
  scale_y_continuous("") +
  scale_x_continuous("Epoch") +
  ggtitle("CoNNOR Training Performance") + mytheme +
  theme(axis.title.y = element_blank(), legend.position = c(1, .5),
        legend.justification = c(1.03, -0.05),
        legend.background = element_rect(fill = "white"))
@

Overfitting occurs when a model learns the training data so well that its understanding of the categories becomes specific to the training cases, which, in turn, leads to poor prediction of new images that were not in the training set. During the training process, overfitting is indicated when the validation loss begins to increase after reaching a global minimum. Alternately, underfitting occurs if the validation accuracy is still increasing when model optimization is terminated (for instance, between epochs 1 and 10 in \autoref{fig:training-accuracy}), because model performance is still improving with continued training. In \autoref{fig:training-accuracy}, validation accuracy levels off after epoch 20, and validation loss has not yet begun to increase at epoch 30, indicating that the model optimization process was halted at an appropriate epoch.

\subsection{Model Accuracy}

\paragraph{Overall Accuracy}
For each image in the test set, the ``true" labels are nine human-assigned labels of 0 or 1 corresponding to the presence or absence of each of the nine shape categories in the image. When the CNN predicts which shapes are in the image, however, it assigns a probability between 0 and 1 for each shape category, with a total of nine probabilities per test image. Thus, to determine the accuracy of the model predictions, we must select a threshold for each label to discretize the model predictions. If the threshold is too low, moderate probabilites will be more likely to exceed the threshold, which increases the true positive and false positive predictions in the test data. Conversely, if the threshold is too high, moderate probabilities will more often fail to meet the threshold, which increases the number of true and false negative predictions. An appropriate threshold must be chosen to produce a high true positive rate while also ideally keeping the false positive rate low. One way to set an optimal threshold is to utilize Receiver Operating Characteristic (ROC) curves, a type of diagnostic plot which compare the false positive rate to the true positive rate for a classification method. \autoref{fig:overall-roc} shows the ROC curve for our model across all classes, and \autoref{fig:class-roc} shows the curve for each class. ROC curves can be summerized by the Area Under the Curve (AUC), which quantifies the overall accuracy of the model. Perfect prediction would be indicated by a right angle along the upper left corner of the plot, with a corresponding AUC of 1, and a diagonal line with an AUC of 0.5 would indicate that the classification method performs no better than random chance.

<<overall-roc, fig.width = 5, fig.height = 5, out.width = ".5\\textwidth", fig.cap = "ROC curve showing overall model performance.", fig.scap = "Overall model performance ROC curve.">>=
library(pROC)
pred_df <- as_tibble(preds) %>% gather(key = feature, value = value)
test_labs_df <- as_tibble(test_labs) %>% gather(key = feature, value = value)
whole_model_roc <- roc(test_labs_df$value, pred_df$value)

whole_model_roc_df <- tibble(tpr = whole_model_roc$sensitivities,
                             fpr = 1 - whole_model_roc$specificities,
                             thresholds = whole_model_roc$thresholds,
                             auc = whole_model_roc$auc[1]) %>%
  nest(tpr, fpr, thresholds, .key =  "roc_plot") %>%
  mutate(eer = purrr::map(roc_plot, eer))
ggplot() +
  geom_line(aes(x = fpr, y = tpr), data = unnest(whole_model_roc_df, roc_plot),
            size = 1.25) +
  geom_label(aes(x = 1, y = .07, label = sprintf("AUC: %0.2f", auc)), hjust = 1,
             vjust = -0.2, data = whole_model_roc_df) +
  geom_point(aes(x = fpr, y = tpr, color = "Equal Error Rate"),
             data = unnest(whole_model_roc_df, eer), size = 2) +
  scale_color_manual("", values = "black") +
  scale_x_continuous("False Positive Rate", breaks = c(0, .25, .5, .75, 1),
                     labels = c("0.0", "", "0.5", "", "1.0")) +
  scale_y_continuous("True Positive Rate", breaks = c(0, .25, .5, .75, 1),
                     labels = c("0.0", "", "0.5", "", "1.0")) +
  ggtitle("CoNNOR Test Set Performance (All Classes)") +
  coord_fixed() + mytheme +
  theme(legend.position = c(1, 0), legend.justification = c(1.01, -0.01),
        legend.title = element_blank(),
        legend.background = element_rect(fill = "white"))
@

<<class-roc, fig.width = 8, fig.height = 6, out.width = "\\textwidth", fig.cap = "Class-by-class ROC curves. AUC is area under the curve, a measure of overall model performance. Equal error rates are marked, indicating the position at which there is equal probability of a false positive or false negative error.", fig.scap = "Class-by-class ROC curves.">>=
aucs <- plot_onehot_roc(preds, test_labs, str_to_title(classes))
thresholds <- purrr::map_dbl(aucs$data$eer, ~.$thresholds)
aucs$data$thresholds <- thresholds

ggplot() +
  geom_line(aes(x = fpr, y = tpr), data = unnest(aucs$data, roc_plot), size = 1.25) +
  geom_label(aes(x = 1, y = 0, label = sprintf("AUC: %0.2f\nEER: %0.2f", auc, thresholds)), hjust = 1, vjust = -0.02, data = aucs$data) +
  geom_point(aes(x = fpr, y = tpr, color = "Equal Error\nRate (EER)"), data = unnest(aucs$data, eer), size = 2.5) +
  scale_color_manual("", values = "black") +
  facet_wrap(~class) +
  scale_x_continuous("False Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  scale_y_continuous("True Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  ggtitle("CoNNOR Test Set Performance") +
  facet_wrap(~class, nrow = 2) +
  coord_fixed() +
  theme(legend.position = c(1, 0), legend.justification = c(1, 0))
@

The full model has an AUC of \Sexpr{round(whole_model_roc_df$auc,2)}, and the AUC for individual classes ranges from \Sexpr{round(min(aucs$data$auc),2)} (for \Sexpr{sprintf("%s",classes[which.min(aucs$data$auc)])}) to \Sexpr{round(max(aucs$data$auc),2)}  (for \Sexpr{sprintf("%s",classes[which.max(aucs$data$auc)])}).
While the class performances do vary slightly, each ROC curve indicates that the model performs significantly better than random chance. The points in \autoref{fig:class-roc} represent the equal error rate (EER), which is the threshold where there is equal probability of a false positive or false negative error. We will use a class specific EER to set an individual threshold for each class, so that any model prediction with probability above the EER for that shape is considered a positive prediction.

\paragraph{Model Predictions---Examples}
The ROC curves provide only a general summary of model accuracy; to learn more about how specific images are classified, we can view images from the test set along with the nine class predictions assigned by the model. \autoref{fig:exemplars} and \autoref{fig:circle-text-consistency} show model predictions for a selection of images. Each image shown in the following examples belongs to either the validation or test data, so none of the images were used during model training. In these figures, the values in each row are the predicted probabilities for the nine classes in the corresponding image, such that each column represents predictions for a given class. The saturation of blue color corresponds to the strength of the predicted probability, and the navy blue border around an image indicates that the predicted probability is above the EER threshold for the given class. Note that the EER is not the same for all classes; in other words, the same probablity value assigned across different categories may yield different conclusions about whether the model predicts the shape is present or absent in the image.

The first rows of \autoref{fig:exemplars} are images which contain only one of the nine classes. Most of the predictions for these images are promising; there are some false-positive labels that indicate confusions between circle/text and polygon/quad, but all of the true shapes are corectly identified by the model above EER. The next image belongs to the ``other" category, as it does not contain any of the nine classes; the model correctly assigns near-zero probabilities to each class. The final rows are of images that belong to more than one class category. Again, predictions are largely accurate, with most of the prominent shapes correctly identified above EER. \fix{Expand on specific misclassifications more, after determining which examples to delete.}

\fix{DC moved to heatmap, green block text could still be removed}

<<eers, include = F>>=
eers <- purrr::map_dbl(aucs$data$eer, function(x){x$thresholds}) %>%
  setNames(classes)
@

<<exemplars, fig.cap = "Examples containing a single shape", fig.pos = "p!", out.width = "100%", cache = F, fig.width = 8, fig.height = 12>>=
single_exemplars <- list.files(file.path(wd, "images", "consistency",
                                         "single-exemplar"), full.names = T)
multi_exemplars <- list.files(file.path(wd, "images", "consistency",
                                        "multi-exemplar"), full.names = T)
pred_prob_plot(rev(c(single_exemplars, multi_exemplars)),
               loaded_model, sort = F, eer = eers)
@

While figure \ref{fig:exemplars} shows representative examples of classifications across multiple shape categories, it is also useful to examine many cases within a given shape to get a sense of model consistency. \autoref{fig:circle-text-consistency} shows a number of images containing circles and/or text. \fix{The top four rows show text contained within a separate, distinct circle shape, and the next five rows show circles represented as the letter ``o" in text. The remaining images contain only one of either text or circles, as well as other possible shape categories.} In particular, the first two of these images contain text that use the capital letter ``G", the next two images contain no circular shapes, and the final \fix{N} images contain pure circles that do not appear to belong to text. The model is able to identify the circle in all cases where a circle is truly present, regardless of whether the circle is part of the text or it is distinct; however, the circle prediction is also strong in three of the four images that do not contain a circle, which indicates that the model is having dificulty distinguishing the two categories. While the circle predictions are above the EER for letters such as C and S, these predictions are generally much lower than those where distinct circles or Os are seen, indicating that there are some indications that features are not as strong in the quasi-circle shapes, but that the model is not able to completely separate Os from C, S, and G shapes which are also very circular. This confusion is complicated by the frequency of images which contain both circles and text: text is present in \Sexpr{100 * round(sum(as.logical(grepl("circle", dfunion$name) & grepl("text", dfunion$name))) / sum(grepl("circle", dfunion$name)), 2)}\% of our labeled images containing circles, and circles are present in \Sexpr{100 * round(sum(as.logical(grepl("circle", dfunion$name) & grepl("text", dfunion$name))) / sum(grepl("text", dfunion$name)), 2)}\% of our images containing text. In addition, the distinction between a circle and an ``o" comes from the context around the shape, which is much more difficult for the model to parse than simply detecting a shape. More importantly, this behavior does not suggest poor model performance as much as a weakness of the classification scheme, which will be discussed in a later section.

% \fix{make sure text matches figure!}

<<circle-text-consistency, fig.cap = "Text with circles or circle-like shapes", fig.pos = "p!", out.width = "100%", cache = T, fig.width = 8, fig.height = 12>>=
circle_text <- list.files(file.path(wd, "images", "consistency", "circle-text"), full.names = T)
pred_prob_plot(rev(circle_text), loaded_model, sort = F, eer = eers)
@

\paragraph{Confusion Matrix}
Figures \ref{fig:exemplars} and \ref{fig:circle-text-consistency} show predictions for a number of individual images. Many of the misclassifications in these examples, however, are not just seen in individual images, but rather exist systematically between certain classes. A confusion matrix is a type of visualization that summarizes the relationships between the labels assigned to an image and the labels predicted by the model across all test data, and is a helpful diagnostic tool to understand which classes are systematically misclassified. The confusion matrix for the fitted model is presented in \autoref{fig:confusion-matrix}. The values along the diagonal of the matrix represent the proportion of true positives captured within each category. The off-diagonal values represent the proportion of false positives for each shape pairing; however, since a single image may truly contain multiple shapes, the values have been adjusted to remove the effect of any true positives from the calculation of false positive proportions. Thus, this might be considered a conditional confusion matrix, because we condition the off diagonal probabilities on the label not being correct. For example, to calculate the proportion of images that contain triangles but are being falsely labeled as containing quadrilaterals, any images that truly contains both triangles and quadrilaterals are removed before calculating the proportion of false quadrilateral labels. As a result of this adjustment, the off-diagonal values of the confusion matrix are the proportion of images within a true class (column-wise) that are falsely labeled as containing another class that has not been labeled in the image (row-wise). Note that this matrix is not symmetric: the proportion of images containing text that are falsely predicted to contain circles is not necessarily equal to the proportion of images containing circles that are falsely predicted to contain text.

<<confusion-matrix, fig.width = 9, fig.height = 8, out.width = "\\textwidth", fig.cap = "Confusion matrix, showing on the diagonal the correct classification rate and on the off-diagonal, classification errors. Note that in multi-label images, correct off-diagonal labels have been excluded from the calculation of false positives.", fig.scap = "Confusion matrix, with correct and incorrect model classifications.", dpi = 600>>=
get_confusion_matrix(predictions = preds, classes = classes,
                     test_labels = test_labs, threshold = thresholds) %>%
  set_names(str_to_title(classes)) %>%
  ggcorrplot(., hc.order = F, outline.col = "white", lab = T) +
  scale_fill_gradient("Classification\nRate", low = "white",
                      high = "cornflowerblue", limits = c(0, 1)) +
  scale_x_discrete("Image Label") + scale_y_discrete("Prediction") +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14, angle = 90, vjust = 1)) +
  ggtitle("CoNNOR Multi-Class Confusion Matrix: Test Set Performance") +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "grey50"),
        panel.grid.minor = element_line(color = "grey60")) +
  theme(plot.margin = grid::unit(c(0,0,0,0), "mm"),
        plot.background = element_rect(fill = "transparent", color = NA),
        plot.subtitle = element_blank(), plot.caption = element_blank(),
        panel.spacing = unit(c(0, 0, 0, 0), "mm"))
@

The horizontal band in \autoref{fig:confusion-matrix} indicates that quadrilaterals are predicted more often than they should be for every true label. Similarly, polygons and triangles produce a moderate number of false positive predictions in other categories, as evidenced by the vertical bands for these categories. It is also apparent that circles and text are commonly confused, which supports our earlier obsesrvartions from \autoref{fig:circle-text-consistency} that these two classes are difficult for the model to distinguish, in part because circles are relatively common components of latin script characters.

\subsection{Model Diagnostics}
After examining model performance, both in specific cases and systematically across classes, there seem to be two primary reasons for misclassifications: poor image contrast, and overlapping features between categories.

\paragraph{Image Contrast}
As described earlier, the images used for this research were obtained from online retailers before being labeled and cropped. One consequence of using outsole images instead of impressions is the impact of colors in the image. Features from outsoles that are primarily black or white in color are typically not as prominent as features from images with more varied color schemes. As a result, the features in low contrast images are not easily resolved by the filters; this is propogated through both model training and prediction of new images. In other words, the model 'learns' that when there are few meaningful features with which to determine which shapes are present, the best strategy for minimizing loss is to predict diffuse probabilities across many classes, as seen in \autoref{fig:low-contrast-consistency}.

<<low-contrast-consistency, fig.cap = "Images with low contrast", fig.pos = "p!", out.width = "100%", cache = F>>=
lowcontrast <- list.files(file.path(wd, "images", "consistency", "low-contrast2"), full.names = T)
pred_prob_plot(lowcontrast, loaded_model, sort = F, eer = eers)
@

\paragraph{Feature Overlap}
We have defined our categories through consultation with practitioners and modifications of previous research, but as a result, there are a number of features (such as lines, angles, and curves) which are not unique to only one category. For example:
\begin{itemize}
\item a triangle may be made up of three acute angles, but a quadrilateral can also contain three acute angles.
\item the only distinction between an ``o" and a circle is whether there is more text around it.
\item the only difference between lines and thin repeating quads are the ends of the shapes.
\end{itemize}

To detect these shapes, the model is doing a significant amount of feature integration beyond just detecting the presence of raw lines, angles, and curves. A correct classification requires the detection and integration of (often) relatively large features (with respect to filters only a few pixels wide); in addition, features can be relatively small relative to the 256x256 image, or may take up the entire image. This wide variation in relative size of the features, combined with the variability of different features across classes, leads to a number of common (but understandable) misclassifications.

This is one area where the CNN's predictions do not match human labels; in part, this is because even with some wider context, the CNN does not use both bottom-up feature integration and top-down object identification based on wider experience - as a result, it cannot easily differentiate between text and constituent shapes within the text.

This issue is exacerbated by the variability of images within a class. A quadrilateral isn't just a square or rectangle with 4 right angles; it can also be a parallelogram, diamond, trapezoid, part of a chevron, or anything else that has four sides. In general, some of our identified shapes are very common on outsoles and appear in our data in many different forms, which further widens the sets of features the model must learn to associate with a single class. Unfortunately, by associating even more possible features with a single category, the number of features that then overlap between categories also increases.

The shape categories with the largest number of images, shown in \autoref{fig:class-characteristic-barchart}, also tend to have the most variability in labeled images. This is supported by \autoref{fig:brand-plots}, which breaks down, by proportion, the top ten most frequent brands in the labeled data for each class. The figure shows that, for example, a relatively large proportion of bowties in our labeled data are contributed by images of Birkenstocks. Since the bowties on Birkenstocks tend to be similar in shape and size, the model is able to create a relatively stable definition of bowties and thus predicts new images of bowties well. There is no similar consistency in categories like quadrilaterals. \autoref{fig:brand-plots} indicates that no one brand makes up more than 4\% of our labeled quadrilateral images. This suggests that the quadrilaterals in our labeled data are more likely to come in a variety of shapes and sizes, so it is more difficult for the model to learn which set or sets of features correspond to a quadrilateral.

<<brand-plots, out.width = "100%", fig.width = 7, fig.height = 5, fig.cap = "Proportions of top 10 shoe brands in our labeled dataset within each shape category, sorted by prevalence. Brand text is present when the brand makes up more than five percent of labeled shoes for the shape category. Some shapes, like bowtie and polygon, are relatively dominated by a small number of brands while other shapes, such as text and quad, are comprised of a large number of brands. Typically, model predictions are best for shape categories without much variability, which, in general, coincides with shapes dominated by a small number of brands.">>=
img_names <- dfunion$filename %>% basename() %>% as.vector

labs <- sapply(classes, grepl, x = str_extract(img_names, "^.*?-\\d?-"))
str <- gsub("^.*?-\\d{0,2}-", "", img_names)

# Extract first, first two, and first three words of string
brand_1 <- str_extract(str, "^.*?-")
brand_2 <- str_extract(str, "^.*?-.*?-")
brand_3 <- str_extract(str, "^.*?-.*?-.*?-")

# These are the beginning of brands that need an extra word extracted
multi_brands <- c("the", "the-north", "polo", "polo-ralph", "5", "5-11",
                  "1", "to", "dr", "la", "new", "old", "under",
                  "steve", "bernie", "cole", "tory",
                  "harley", "kristin", "eric",
                  "spring", "chinese", "dirty")

brand <- ifelse(brand_1 %in% paste0(multi_brands, "-"),
                ifelse(brand_2 %in% paste0(multi_brands, "-"),
                       brand_3, brand_2),
                brand_1) %>%
  gsub(. , pattern = "-", replacement = " ") %>%
  trimws(., "right"); rm(brand_1, brand_2, brand_3, multi_brands)

df_images <- cbind(tibble(str, brand), labs)

df_shoes <- df_images %>%
  group_by(str, brand) %>%
  summarize_at(vars(bowtie:triangle), any)

df_shoes_long <- df_shoes %>%
  tidyr::gather(key = feature, value = value, -str, -brand) %>%
  ungroup() %>%
  tidyr::nest(c(str, value)) %>%
  mutate(n_true = purrr::map_int(data, ~sum(.$value)),
         n_shoes_brand = purrr::map_int(data, ~length(.$value)))

brand_prop_by_feature <- df_shoes_long %>%
  group_by(feature) %>%
  mutate(prop = n_true/sum(n_true)) %>%
  arrange(feature, desc(prop)) %>%
  mutate(brand_cat = ifelse(row_number() <= 10, brand, "other")) %>%
  group_by(feature) %>%
  mutate(rank = pmin(row_number(), 11))

brand_prop_by_feature_10 <- brand_prop_by_feature %>%
  group_by(feature, brand_cat, rank) %>%
  summarize(prop = sum(prop)) %>%
  ungroup() %>%
  group_by(feature) %>%
  arrange(feature, rank) %>%
  mutate(cprop = cumsum(prop),
         cprop2 = lag(cprop, 1, default = 0),
         mid = (cprop + cprop2)/2,
         rank = ifelse(rank == 11, NA, rank))

ggplot(data = brand_prop_by_feature_10) +
  geom_bar(aes(x = feature, y = prop, fill = factor(rank)),
           position = "stack", stat = "identity", color = "black") +
  geom_text(aes(x = feature, y = 1-mid, label = brand_cat),
            data = filter(brand_prop_by_feature_10, prop > .05), size = 2.5) +
  scale_fill_brewer(palette = "Paired", guide = F, na.value = "grey") +
  scale_y_continuous("Proportion") + scale_x_discrete("") +
  ggtitle("Proportion of Labeled Features by Brand")
@

We have proposed several hypotheses to explain why the model's predictions are not entirely consistent with manual labels on the images. We can test these hypotheses by examining what the model ``sees" when making its predictions \mt{\sout{using heatmaps, or class activation maps (CAMs)}}. Heatmaps, also known as Class Activation Maps \mt{(CAMs)}, are a visual diagnostic tool that highlights the areas of an image that are most significant to classification for a given class \citep{chollet_allaire_2018}. Using these graphics, we can identify which locations in an image contribute most to the model's predictions. In figures \fix{X-Y}, heatmaps are shown for a selection of \fix{three} classes, which are either contained in the image or have high output probabilities, alongside the original image. In these heatmaps, we can confirm that multiple classes have common features, resulting in misclassifications.

\fix{Choose three (or four) heatmaps:
\begin{itemize}
\item An image with multiple true shapes that the model clearly predicts well, to prove that it has learned the features we think it should (probably an UGG logo)
\item (Seychelles circle/text confusion)
\item An image with high quad/tri/poly confusion, but the model uses appropriate features to make educated guesses
\item An image with no true classes but would yield false positives (e.g., corks, puzzle pieces) to show that there will always be weird shapes that our classification can't be expected to cover, but our model will still behave reasonably enough when encountering these cases.
\end{itemize}}

<<generate-heatmaps, echo = F, eval = F>>=
# My computer has troubles with the transparency of the overlay...
# This code is functional, but highest quality will come from generating these
# images on Bigfoot and loading them statically.

h_imgdir <- file.path(imgdir, "heatmaps", "processed")
if(!dir.exists(h_imgdir)) {dir.create(h_imgdir)}

# Turn all images in "heatmaps/raw" to top-4 heatmaps
imgs <- list.files(file.path(wd, "images", "heatmaps", "raw"), full.names = T)
sapply(imgs, function(x) {
  calc_heatmap(x, loaded_model) %>%
    prune_heatmap(top_n = 4, sort = "value") %>%
    create_composite(save_file = T, outdir = h_imgdir,
                     color_by_label = F, eer = eers)
})

# calc_heatmap(imgs[which(grepl("Ugg-grey", imgs))], loaded_model) %>%
#   prune_heatmap(top_n = 3, sort = "value") %>%
#   create_composite(save_file = T, outdir = h_imgdir, color_by_label = F,
#                    eer = eers)
#
# calc_heatmap(imgs[which(grepl("text", imgs))], loaded_model) %>%
#   prune_heatmap(top_n = 3, sort = "value") %>%
#   create_composite(save_file = T, outdir = h_imgdir, color_by_label = F,
#                    eer = eers)
#
# calc_heatmap(imgs[which(grepl("polygon_recolor", imgs))], loaded_model) %>%
#   prune_heatmap(top_n = 3, sort = "value") %>%
#   create_composite(save_file = T, outdir = h_imgdir, color_by_label = F,
#                    eer = eers)
#
# calc_heatmap(imgs[which(grepl("corks", imgs))], loaded_model) %>%
#   prune_heatmap(top_n = 3, sort = "value") %>%
#   create_composite(save_file = T, outdir = h_imgdir, color_by_label = F,
#                    eer = eers)
#
# calc_heatmap(imgs[which(grepl("chevron-circle", imgs))], loaded_model) %>%
#   #prune_heatmap(c("chevron", "quad", "circle")) %>%
#   create_composite(save_file = T, outdir = h_imgdir, color_by_label = F)
#
# calc_heatmap(imgs[which(grepl("triangle", imgs))], loaded_model) %>%
#   #prune_heatmap(c("triangle", "quad", "chevron")) %>%
#   create_composite(save_file = T, outdir = h_imgdir, color_by_label = F)
#
# calc_heatmap(imgs[which(grepl("star", imgs))], loaded_model) %>%
#   #prune_heatmap(c("star", "bowtie", "polygon")) %>%
#   create_composite(save_file = T, outdir = h_imgdir, color_by_label = F)
#
# calc_heatmap(imgs[which(grepl("other", imgs))], loaded_model) %>%
#   #prune_heatmap(c("star", "chevron", "polygon")) %>%
#   create_composite(save_file = T, outdir = h_imgdir, color_by_label = F)

@

<<current-heatmaps, out.width = ".95\\textwidth", fig.width = 8, fig.height = 6, fig.cap = c("A section of an Ugg shoe containing the logo. The model correctly identifies triangles and text (and references the correct portion of the image in both cases). The model identifies a circle as well; while it could be argued that the triangles form an implicit circle, the heatmap demonstrates that the circular portion of the image is the 'G', which would not be a closed figure labeled as a circle. The model's prediction of the presence of a circle is understandable, but incorrect.", "In this image of a Seychelles shoe, the model identifies text and circle. The prediction of text is correct; the prediction of circle is not - there are very round shapes, but they do not form a closed figure that could be identified as a circle.", "polygon", "corks"), echo = F, message = F, warning = F, fig.pos = "!p">>=

h_imgdir <- file.path(imgdir, "heatmaps", "processed")
heatmaps <- list.files(h_imgdir, pattern = "^heatmap", full.names = T)

knitr::include_graphics(heatmaps[grepl("Ugg-grey", heatmaps)])
knitr::include_graphics(heatmaps[grepl("text", heatmaps)])
knitr::include_graphics(heatmaps[grepl("polygon_recolor", heatmaps)])
knitr::include_graphics(heatmaps[grepl("corks", heatmaps)])

#sapply(heatmaps, knitr::include_graphics)
@

<<chevron-circle-heatmap, eval = F, out.width = ".95\\textwidth", fig.width = 8, fig.height = 6, fig.scap = "Heatmaps of an image containing chevrons and circles.", fig.cap = "Chevrons are identified by sharp corners and parallel lines, but the thick line between the two shape sections is falsely identified as a quadrilateral.", fig.pos = "!h">>=
heatmaps <- list.files(file.path(wd, "images", "heatmaps", "static"),
                       pattern = "^heatmap", full.names = T)
knitr::include_graphics(heatmaps[grepl("chevron-circle", heatmaps)])
@

<<text-seychelles-heatmap, eval = F, out.width = ".95\\textwidth", fig.width = 8, fig.height = 6, fig.scap = "Heatmaps of an image containing text.", fig.cap = "This heatmap illustrates that the high probability of a circle is a result of the highly curled tails of the 'S' in the text.">>=
knitr::include_graphics(heatmaps[grepl("text", heatmaps)])
@

<<corks-heatmap, eval = F, out.width = ".95\\textwidth", fig.width = 8, fig.height = 6, fig.scap = "Heatmaps of an image containing none of the target classes.", fig.cap = "This outsole shows repeated outlines of corks, which do not fall into any of the nine classes; the concave corners where the corks overlap are predicted to be stars, and the round edges lead to predictions of circles.\\svp{Not really... only 0.03 probability}">>=
knitr::include_graphics(heatmaps[grepl("corks", heatmaps)])
@

<<puzzle-heatmap, eval = F, out.width = ".95\\textwidth", fig.width = 8, fig.height = 6, fig.scap = "Heatmaps of an image containing none of the target classes.", fig.cap = "This outsole shows repeated outlines of puzzle pieces, which do not fall into any of the nine classes.">>=
knitr::include_graphics(heatmaps[grepl("other", heatmaps)])
@


\section{Conclusions}
\svp{Rework this to address 3 main conclusions - 1: NNs work for automatic feature ID, 2: Different levels of feature variability within classes, and 3: correcting for contrast is important.}

% Restate goal and briefly summarize methods
The goal of this research was to develop a method to automatically identify geometric class characteristics of shoe outsoles. A set of geometric class characteristics was defined, based on feature sets currently used by examiners, to both broadly classify a large variety of shoes and to narrow down similarity into a manageable number and type of features for further use in a shoeprint analysis. Thousands of outsole images were obtained from online shoe retailers and labeled according to their geometric shapes. The pre-trained convolutional base of VGG16 was then used, with a new classifier trained on a portion of these labeled images, to output predicted probabilities for test images within the defined set of geometric features. After training, the model performs well on the data provided. In general, the model is able to identify many well-defined geometric shapes in the images.

% Discussion of weaknesses
Although model predictions are generally accurate, there are a number of systematic misclassifications. The model has substantial difficulty predicting images with low contrast because image features are obscured. Exploring methods of color correction, such as histogram equalization, may prove useful for eliminating the effect of contrast on predictions.

Another source of systematic misclassifications stems from the inherent overlap between features of common geometric shapes; since CNNs make predictions via bottom-up feature integration, it is more difficult for the model to distinguish between geometrically similar shapes, such as chevrons and triangles. This issue is exacerbated by high variability of those shapes on outsoles, which requires the model to form a flexible understanding of each shape category, making distinction between similar shapes even more difficult. These issues of overlap and variability are at least in part due to our decision to use the features used by practitioners directly; some of this may also result from our operational definitions of various shapes, which did not take into account the model's capabilities and initial training. While a \fix{full} remedy may not be possible, there may be some modifications to operational class definitions that we can make to optimize labels for model prediction.







%\section{Future Work}
%\svp{Need to show image contrast stuff to use this paragraph.}
%Although CoNNOR performs well in its current state, there are still a number of ways to potentially improve prediction accuracy. For one, image contrast  plays a large role in how well the model classifies the geometric shapes present. Thus, exploring methods of color correction, such as histogram normalization, may prove useful for eliminating the effect of contrast on predictions. Geometric features are also relatively simple with respect to the features that are being detected by the final convolutional block of VGG16, as in \autoref{fig:vgg16layers}; it is quite possible that prediction accuracy could improve by directly classifying the features that are output by the fourth block, rather than using the full convolutional base.


\bibliography{mybibfile}

\end{document}